{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Install Pacakges</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:45:45.266838Z",
     "iopub.status.busy": "2024-12-09T05:45:45.266370Z",
     "iopub.status.idle": "2024-12-09T05:46:14.685176Z",
     "shell.execute_reply": "2024-12-09T05:46:14.683826Z",
     "shell.execute_reply.started": "2024-12-09T05:45:45.266799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets lxml TinyImageNet --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Import Libraries</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:48:24.941604Z",
     "iopub.status.busy": "2024-12-09T05:48:24.941087Z",
     "iopub.status.idle": "2024-12-09T05:48:25.126240Z",
     "shell.execute_reply": "2024-12-09T05:48:25.125344Z",
     "shell.execute_reply.started": "2024-12-09T05:48:24.941557Z"
    },
    "id": "IWgcTDs4vXBk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import tarfile\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from numba import cuda\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine, euclidean, jensenshannon\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tinyimagenet import TinyImageNet\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.model_zoo import tqdm\n",
    "from torchvision.datasets import (CIFAR10, CIFAR100, MNIST, STL10, SVHN, DatasetFolder, FashionMNIST, ImageFolder)\n",
    "from torchvision.datasets.utils import (check_integrity,\n",
    "                                        download_file_from_google_drive)\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Garbage Collection</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before memory cleaning:\n",
      "\n",
      "Allocated memory: 0.00 MB\n",
      "Cached memory: 0.00 MB\n",
      "after memory cleaning:\n",
      "\n",
      "Allocated memory: 0.00 MB\n",
      "Cached memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "def print_gpu_memory():\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "\n",
    "print(\"before memory cleaning:\\n\")\n",
    "print_gpu_memory()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "\n",
    "print(\"after memory cleaning:\\n\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# ----------- manually clear memory in case of any error\n",
    "#!sudo fuser -v /dev/nvidia* or nvidia-smi\n",
    "# remove all python process ids from gpu\n",
    "#!sudo kill -9 PID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Make Directories</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:34.456780Z",
     "iopub.status.busy": "2024-12-09T05:33:34.456325Z",
     "iopub.status.idle": "2024-12-09T05:33:38.523961Z",
     "shell.execute_reply": "2024-12-09T05:33:38.522694Z",
     "shell.execute_reply.started": "2024-12-09T05:33:34.456716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "!mkdir models/before_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:38.525906Z",
     "iopub.status.busy": "2024-12-09T05:33:38.525588Z",
     "iopub.status.idle": "2024-12-09T05:33:38.531683Z",
     "shell.execute_reply": "2024-12-09T05:33:38.530748Z",
     "shell.execute_reply.started": "2024-12-09T05:33:38.525874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "log_path = datetime.now().strftime(\"%Y-%m-%d_%H\")\n",
    "log_file = log_path + \".log\"\n",
    "open(log_file, \"a\").close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Configs</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:38.533041Z",
     "iopub.status.busy": "2024-12-09T05:33:38.532778Z",
     "iopub.status.idle": "2024-12-09T05:33:38.790687Z",
     "shell.execute_reply": "2024-12-09T05:33:38.789843Z",
     "shell.execute_reply.started": "2024-12-09T05:33:38.533002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "sns.set_theme(\n",
    "    style=\"darkgrid\", font_scale=1.5, font=\"SimHei\", rc={\"axes.unicode_minus\": False}\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERING_PERIOD = 5           # Set to `1` to run simple Federated Learning with out clustering\n",
    "FEDERATED_LEARNING_ROUNDS = 6   # The round in with Federated Learning will be executed\n",
    "\"\"\"\n",
    "|MODEL_TYPE |  DATASET_TYPE    | NUMBER_OF_CLASSES| PARTITION      | ROUND_EPOCHS|\n",
    "|-----------|------------------|------------------|----------------|-------------|\n",
    "|cnn        |  fmnist          | 10               | noniid-#label2 | 1           |\n",
    "|resnet18   |  cifar10         | 10               | noniid-#label2 | 1           |\n",
    "|mobilenet  |  svhn            | 10               | noniid-#label2 | 1           |\n",
    "|vgg16      |  stl10           | 10               | noniid-#label2 | ?           |\n",
    "|alexnet    |  tinyimagenet    | 200              | noniid-#label40| 10          |\n",
    "|-----------|------------------|------------------|----------------|-------------|\n",
    "\"\"\"\n",
    "MODEL_TYPE = \"alexnet\"\n",
    "DATASET_TYPE = \"tinyimagenet\"\n",
    "# by default set to 0.001 and for AlexNet set to 0.0001\n",
    "LEARNING_RATE = 0.0001\n",
    "NUMBER_OF_CLASSES = 200\n",
    "NUMBER_OF_CLIENTS = 10\n",
    "# the second part accepted format is: \"labeldir\" (Dirichlet) or \"#label20\"\n",
    "PARTITION = \"noniid-\" + \"#label40\"\n",
    "# set to 10 for AlexNet\n",
    "ROUND_EPOCHS = 10\n",
    "SENSITIVITY_PERCENTAGE = 0.1\n",
    "\"\"\"\n",
    "DISTANCE_METRIC values are:\n",
    "- coordinate\n",
    "- cosine\n",
    "- euclidean\n",
    "- jensen-shannon\n",
    "- wasserstein\n",
    "\"\"\"\n",
    "DISTANCE_METRIC = \"coordinate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Model Network</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.351995Z",
     "iopub.status.busy": "2024-12-09T05:33:39.350967Z",
     "iopub.status.idle": "2024-12-09T05:33:39.367653Z",
     "shell.execute_reply": "2024-12-09T05:33:39.366717Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.351954Z"
    },
    "id": "evEmrviBwIoH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        if MODEL_TYPE == \"resnet18\":\n",
    "            self.resnet18 = models.resnet18(pretrained=False)\n",
    "            if DATASET_TYPE == \"mnist\":\n",
    "                self.resnet18.conv1 = nn.Conv2d(\n",
    "                    1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "                )\n",
    "            self.resnet18.fc = nn.Linear(\n",
    "                self.resnet18.fc.in_features, NUMBER_OF_CLASSES\n",
    "            )\n",
    "\n",
    "        elif MODEL_TYPE == \"cnn\":\n",
    "            self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "            self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        elif MODEL_TYPE == \"mobilenet\":\n",
    "            self.mobilenet = models.mobilenet_v2(pretrained=False)\n",
    "            self.mobilenet.classifier[1] = nn.Linear(\n",
    "                self.mobilenet.last_channel, NUMBER_OF_CLASSES\n",
    "            )\n",
    "\n",
    "        elif MODEL_TYPE == \"vgg16\":\n",
    "            self.vgg16 = models.vgg16(pretrained=False)\n",
    "            self.vgg16.classifier[6] = nn.Linear(\n",
    "                self.vgg16.classifier[6].in_features, NUMBER_OF_CLASSES\n",
    "            )\n",
    "\n",
    "        elif MODEL_TYPE == \"alexnet\":\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(256 * 6 * 6, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(512, NUMBER_OF_CLASSES),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = None\n",
    "\n",
    "        if MODEL_TYPE == \"resnet18\":\n",
    "            out = self.resnet18(x)\n",
    "        elif MODEL_TYPE == \"cnn\":\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = x.view(x.size(0), 16 * 5 * 5)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            out = x\n",
    "\n",
    "        elif MODEL_TYPE == \"mobilenet\":\n",
    "            out = self.mobilenet(x)\n",
    "\n",
    "        elif MODEL_TYPE == \"vgg16\":\n",
    "            out = self.vgg16(x)\n",
    "\n",
    "        elif MODEL_TYPE == \"alexnet\":\n",
    "            x = self.features(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.classifier(x)\n",
    "            out = x\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Learning</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.370196Z",
     "iopub.status.busy": "2024-12-09T05:33:39.369557Z",
     "iopub.status.idle": "2024-12-09T05:33:39.387952Z",
     "shell.execute_reply": "2024-12-09T05:33:39.386804Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.370159Z"
    },
    "id": "cCM4LUpzwTXE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "def train(net, node_id, train_loader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-7,\n",
    "        weight_decay=1e-4,\n",
    "    )\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "    loss /= len(train_loader.dataset)\n",
    "    acc = correct / total\n",
    "    # ! CEMENTED‌ TO‌ SAVE‌ DISK‌SPACE\n",
    "    # model_path = f\"models/node_{node_id}.pth\"\n",
    "    # torch.save(net.state_dict(), model_path)\n",
    "    return acc, loss\n",
    "\n",
    "\n",
    "def test(net, test_loader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Client</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.390658Z",
     "iopub.status.busy": "2024-12-09T05:33:39.390325Z",
     "iopub.status.idle": "2024-12-09T05:33:39.416416Z",
     "shell.execute_reply": "2024-12-09T05:33:39.414449Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.390620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, net, node_id, train_loader, test_loader):\n",
    "        self.net = net.to(DEVICE)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.node_id = node_id\n",
    "        self.train_acc, self.test_acc = 0.0, 0.0\n",
    "        self.global_net = Net().to(DEVICE)\n",
    "\n",
    "    def set_bias(self, pref, bias):\n",
    "        self.bias = bias\n",
    "        self.pref = pref\n",
    "\n",
    "    def set_shard(self, shard):\n",
    "        self.shard = shard\n",
    "\n",
    "    def get_global_net(self):\n",
    "        return self.global_net\n",
    "\n",
    "    def setting_parameters(self, parameters: List[np.ndarray]):\n",
    "        params_dict = zip(self.net.state_dict().items(), parameters)\n",
    "        state_dict = OrderedDict(\n",
    "            {k: torch.Tensor(v).to(DEVICE) for k, v in params_dict}\n",
    "        )\n",
    "        self.net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def getting_parameters(self) -> List[np.ndarray]:\n",
    "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
    "\n",
    "    def fit(self, parameters):\n",
    "        self.setting_parameters(parameters)\n",
    "        train(self.net, self.node_id, self.train_loader, epochs=ROUND_EPOCHS)\n",
    "        return self.getting_parameters(), len(self.train_loader), {}\n",
    "\n",
    "    def evaluate(self, parameters):\n",
    "        self.setting_parameters(parameters)\n",
    "        loss, accuracy = test(self.net, self.test_loader)\n",
    "        return float(loss), len(self.test_loader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "    def Train_test_and_return_acc(self):\n",
    "        self.train_acc, _ = train(self.net, self.node_id, self.train_loader, ROUND_EPOCHS)\n",
    "        self.test_acc, _ = test(self.net, self.test_loader)\n",
    "        return self.train_acc, self.test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Server</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.420149Z",
     "iopub.status.busy": "2024-12-09T05:33:39.418276Z",
     "iopub.status.idle": "2024-12-09T05:33:39.455212Z",
     "shell.execute_reply": "2024-12-09T05:33:39.453215Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.420094Z"
    },
    "id": "0SW7jKZNwbhJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def divide_nested_list(nested_list, divisor):\n",
    "    for i in range(len(nested_list)):\n",
    "        if isinstance(nested_list[i], list):\n",
    "            divide_nested_list(nested_list[i], divisor)\n",
    "        else:\n",
    "            nested_list[i] /= divisor\n",
    "    return nested_list\n",
    "\n",
    "\n",
    "def zero_nested_list(nested_list):\n",
    "    for i in range(len(nested_list)):\n",
    "        if isinstance(nested_list[i], list):\n",
    "            zero_nested_list(nested_list[i])\n",
    "        else:\n",
    "            nested_list[i] = 0\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def append_model(self, model: nn.Module):\n",
    "        if not isinstance(model, nn.Module):\n",
    "            raise TypeError(\"Only instances of nn.Module can be appended\")\n",
    "        self.models.append(model)\n",
    "\n",
    "    def aggregate(self):\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models added to the server.\")\n",
    "        print(\"model numbers:\", len(self.models))\n",
    "        device = next(self.models[0].parameters()).device\n",
    "        for model in self.models:\n",
    "            model.to(device)\n",
    "        avg_model = Net().to(device)\n",
    "        with torch.no_grad():\n",
    "            for param_name, avg_param in avg_model.named_parameters():\n",
    "                temp = torch.zeros_like(avg_param)\n",
    "                for model in self.models:\n",
    "                    model_param = dict(model.named_parameters())[param_name]\n",
    "                    temp += model_param.data\n",
    "                avg_param.copy_(temp / len(self.models))\n",
    "        return avg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Clustering</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.457583Z",
     "iopub.status.busy": "2024-12-09T05:33:39.457181Z",
     "iopub.status.idle": "2024-12-09T05:33:39.544421Z",
     "shell.execute_reply": "2024-12-09T05:33:39.543934Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.457534Z"
    },
    "id": "asSZtfDAwmrd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_num_cluster(clusters):\n",
    "    num_cluster = []\n",
    "    for item in clusters:\n",
    "        if item not in num_cluster:\n",
    "            num_cluster.append(item)\n",
    "    return len(num_cluster)\n",
    "\n",
    "\n",
    "class Clustering:\n",
    "    def __init__(self, clients, trainLoaders, percentage):\n",
    "        self.clients = clients\n",
    "        self.num_nodes = len(clients)\n",
    "        self.percentage = percentage\n",
    "        self.Mask_Number = 0\n",
    "        self.maskIds = []\n",
    "        self.grads = []\n",
    "        self.load_and_calculate_sensitivity(trainLoaders)\n",
    "        self.distances = self.calculate_distance()\n",
    "        self.Clusters = self.make_clusters()\n",
    "\n",
    "    def assign_save_ids_to_weights(self, model):\n",
    "        weight_id_map = {}\n",
    "        weight_id = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            weight_id_map[name] = {}\n",
    "            num_weights = parameter.numel()\n",
    "            for i in range(num_weights):\n",
    "                weight_id_map[name][i] = weight_id\n",
    "                weight_id += 1\n",
    "        filename = \"weight_to_id.csv\"\n",
    "        if not os.path.exists(filename):\n",
    "            with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\"Layer\", \"Weight Index\", \"Weight ID\"])\n",
    "                for layer_name, indices in weight_id_map.items():\n",
    "                    for index, weight_id in indices.items():\n",
    "                        writer.writerow([layer_name, index, weight_id])\n",
    "        return weight_id_map\n",
    "\n",
    "    def load_and_calculate_sensitivity(self, trainLoaders):\n",
    "        \"\"\"\n",
    "        Calculate sensitivity for each client and store the results in the object.\n",
    "        \"\"\"\n",
    "        for cid in self.clients:\n",
    "            model = load_torch_model(cid).to(DEVICE)\n",
    "            sensitivity_value = self.calculate_sensitivity(\n",
    "                model, trainLoaders[int(cid)]\n",
    "            )\n",
    "            weight_id_map = self.assign_save_ids_to_weights(\n",
    "                load_torch_model(0).to(DEVICE)\n",
    "            )\n",
    "            \n",
    "            mask_ID, weights = self.get_maskIds(sensitivity_value, weight_id_map, self.percentage)   # top sensitive weights will filter here\n",
    "            print(f\"Model weights and sensitivity data for client #{cid} processed.\")\n",
    "\n",
    "            self.maskIds.append(mask_ID)\n",
    "            self.grads.append(weights)\n",
    "\n",
    "    def calculate_sensitivity(self, model, dataloader):\n",
    "        model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        gradient_sums = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            gradient_sums[name] = 0.0\n",
    "            param.requires_grad_(True)\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sensitivities = {}\n",
    "            for name, parameter in model.named_parameters():\n",
    "                grads = parameter.grad.abs().view(-1).cpu().numpy()\n",
    "                for i, grad in enumerate(grads):\n",
    "                    sensitivities[(name, i)] = grad\n",
    "            return sensitivities\n",
    "\n",
    "    def get_maskIds(self, sensitivity_values_node, weight_id_map, sensitive_percentage):\n",
    "        num_weights = len(sensitivity_values_node)\n",
    "        top_k = int(np.ceil(sensitive_percentage * num_weights / 100))\n",
    "        self.Mask_Number = top_k\n",
    "        sorted_weights = sorted(\n",
    "            sensitivity_values_node.items(), key=lambda item: item[1], reverse=True\n",
    "        )[:top_k]\n",
    "        weights = [weight for (layer, index), weight in sensitivity_values_node.items()]\n",
    "        top_weight_ids = [\n",
    "            weight_id_map[layer][index] for (layer, index), _ in sorted_weights\n",
    "        ]\n",
    "        return top_weight_ids, weights\n",
    "\n",
    "    def normalize(self, distances, sensitive):\n",
    "        normal_distances = np.zeros((self.num_nodes, self.num_nodes))\n",
    "        for i in range(self.num_nodes):\n",
    "            normal_distances[i][i] = 0\n",
    "            for j in range(i + 1, self.num_nodes):\n",
    "                normal_distances[i][j] = normal_distances[j][i] = distances[i][j] / len(\n",
    "                    sensitive\n",
    "                )\n",
    "        return normal_distances\n",
    "\n",
    "    def calculate_common_ids(self, index1, index2):\n",
    "        arr1 = self.maskIds[index1]\n",
    "        arr2 = self.maskIds[index2]\n",
    "        sarr1 = set(arr1)\n",
    "        sarr2 = set(arr2)\n",
    "        inter = sarr1.intersection(sarr2)\n",
    "        similarity1 = len(inter)\n",
    "        return similarity1\n",
    "\n",
    "    def calculate_distance(\n",
    "        self,\n",
    "    ):\n",
    "        similarity_matrix = np.zeros((self.num_nodes, self.num_nodes))\n",
    "        \n",
    "        for i in range(self.num_nodes):\n",
    "            for j in range(i + 1, self.num_nodes):\n",
    "                \n",
    "                if DISTANCE_METRIC == \"coordinate\":\n",
    "                    similarity = self.calculate_common_ids(i, j)\n",
    "                elif DISTANCE_METRIC == \"cosine\":\n",
    "                    # Cosine distance\n",
    "                    similarity = 1 - cosine(self.grads[i], self.grads[j])\n",
    "                elif DISTANCE_METRIC == \"euclidean\":\n",
    "                    # Euclidean distance\n",
    "                    similarity = -euclidean(self.grads[i], self.grads[j])  # Negative for clustering\n",
    "                elif DISTANCE_METRIC == \"jensen-shannon\":\n",
    "                    # Jensen-Shannon divergence\n",
    "                    similarity = -jensenshannon(self.grads[i], self.grads[j])  # Negative for clustering\n",
    "                elif DISTANCE_METRIC == \"wasserstein\":\n",
    "                    # Wasserstein distance\n",
    "                    similarity = -wasserstein_distance(self.grads[i], self.grads[j])  # Negative for clustering\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported distance metric: {DISTANCE_METRIC}\")\n",
    "                similarity_matrix[i, j] = similarity\n",
    "                similarity_matrix[j, i] = similarity\n",
    "            similarity_matrix[i, i] = self.Mask_Number\n",
    "        distances = self.Mask_Number - similarity_matrix\n",
    "        \n",
    "        self.save_distances_to_csv(distances)\n",
    "        \n",
    "        return distances\n",
    "\n",
    "    def index_to_value(self, groups):\n",
    "        value_groups = []\n",
    "        for group in groups:\n",
    "            list1 = []\n",
    "            for index in group:\n",
    "                list1.append(self.clients[index])\n",
    "            value_groups.append(list1)\n",
    "        return value_groups\n",
    "\n",
    "    def make_clusters(self):\n",
    "        normal_distances = (self.distances + self.distances.T) / 2\n",
    "        np.fill_diagonal(normal_distances, 0)\n",
    "        affinity_propagation = AffinityPropagation(affinity=\"precomputed\")\n",
    "        normal_distances = -normal_distances\n",
    "        clusters = affinity_propagation.fit_predict(normal_distances)\n",
    "        print(f\"cluster results:{clusters}\")\n",
    "        # Find the maximum cluster label from the assigned labels\n",
    "        max_label = max(clusters)\n",
    "        # Assign unique positive labels to noise points (initially labeled as -1)\n",
    "        noise_indices = clusters == -1\n",
    "        unique_noise_labels = np.arange(\n",
    "            max_label + 1, max_label + 1 + np.sum(noise_indices)\n",
    "        )\n",
    "        clusters[noise_indices] = unique_noise_labels\n",
    "        cluster_list = [\n",
    "            np.where(clusters == cluster_id)[0].tolist()\n",
    "            for cluster_id in range(find_num_cluster(clusters))\n",
    "        ]\n",
    "        cluster_list = self.index_to_value(cluster_list)\n",
    "        return cluster_list\n",
    "    \n",
    "    def save_distances_to_csv(self, distances):\n",
    "        \"\"\"\n",
    "        Save the distance matrix to a CSV file.\n",
    "        \"\"\"\n",
    "        filename = f\"distances_{DISTANCE_METRIC}.csv\"\n",
    "        with open(filename, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Node\"] + [f\"Node_{i}\" for i in range(self.num_nodes)])\n",
    "            for i, row in enumerate(distances):\n",
    "                writer.writerow([f\"Node_{i}\"] + row.tolist())\n",
    "\n",
    "        print(f\"Distance matrix saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Federated Learning</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.546365Z",
     "iopub.status.busy": "2024-12-09T05:33:39.546049Z",
     "iopub.status.idle": "2024-12-09T05:33:39.562566Z",
     "shell.execute_reply": "2024-12-09T05:33:39.561764Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.546330Z"
    },
    "id": "DyY6UPxMw0EA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FL:\n",
    "    def __init__(\n",
    "        self,\n",
    "        clients,\n",
    "        client_initial_models,\n",
    "        round_number,\n",
    "        train_loaders,\n",
    "        test_loaders,\n",
    "        SENSITIVITY_PERCENTAGE,\n",
    "    ):\n",
    "        self.clients = clients\n",
    "        self.NUMBER_OF_CLIENTS = len(clients)\n",
    "        self.client_initial_models = client_initial_models\n",
    "        self.SENSITIVITY_PERCENTAGE = SENSITIVITY_PERCENTAGE\n",
    "        self.train_loaders = train_loaders\n",
    "        self.test_loaders = test_loaders\n",
    "        self.round_number = round_number\n",
    "        self.global_model = None\n",
    "        self.clustering_result = None\n",
    "        self.client_obj_list = []\n",
    "        self.accuracies = {}\n",
    "        self.training()\n",
    "\n",
    "    def training(self):\n",
    "        for cid in self.clients:\n",
    "            print(\"cid is:\", cid)\n",
    "            client = Client(\n",
    "                self.client_initial_models[self.clients.index(int(cid))],\n",
    "                cid,\n",
    "                self.train_loaders[int(cid)],\n",
    "                self.test_loaders[int(cid)],\n",
    "            )\n",
    "            self.client_obj_list.append(client)\n",
    "        global_model = Net()\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        start_time = datetime.now()\n",
    "        for r in range(self.round_number):\n",
    "            print(f\"\\nRound {r+1}/{self.round_number}\")\n",
    "            server = Server()\n",
    "            global_accuracy = 0\n",
    "            for cid in self.clients:\n",
    "                train_acc, test_acc = self.client_obj_list[\n",
    "                    self.clients.index(cid)\n",
    "                ].Train_test_and_return_acc()\n",
    "                print(\n",
    "                    \"_____________________________________________________________________________________________________________\"\n",
    "                )\n",
    "                print(f\"node {cid}: train_acc: {train_acc}, test_acc:{test_acc}\")\n",
    "                with open(log_file, \"a\") as f:\n",
    "                    f.write(\n",
    "                        f\"\\nNode {cid} - Round {r+1}: Train Accuracy: {train_acc}%, Test Accuracy: {test_acc}%\"\n",
    "                    )\n",
    "                global_accuracy += test_acc\n",
    "                server.append_model(self.client_obj_list[self.clients.index(cid)].net)\n",
    "            global_model = server.aggregate()\n",
    "            # global_model = server.aggregate_prox(global_model)\n",
    "            end_time = datetime.now()\n",
    "            execution_time = end_time - start_time\n",
    "            print(\"time\", execution_time)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"\\n Exe FL Round Time: {execution_time}\")\n",
    "            # global_model, c = server.aggregate_scaffold(global_model, client_controls, c)\n",
    "            print(\"global acc:\", global_accuracy / self.NUMBER_OF_CLIENTS)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(\n",
    "                    f\"\\nGlobal Model of {self.NUMBER_OF_CLIENTS}- Round {r+1}: Test Accuracy is: {global_accuracy/self.NUMBER_OF_CLIENTS}%\"\n",
    "                )\n",
    "            for cid in self.clients:\n",
    "                model_path = f\"models/before_aggregation/node_{cid}.pth\"\n",
    "                torch.save(\n",
    "                    self.client_obj_list[self.clients.index(cid)].net.state_dict(),\n",
    "                    model_path,\n",
    "                )\n",
    "                self.client_obj_list[self.clients.index(cid)].net = copy.deepcopy(\n",
    "                    global_model\n",
    "                )\n",
    "        self.global_model = global_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Loading & Saving</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.564155Z",
     "iopub.status.busy": "2024-12-09T05:33:39.563846Z",
     "iopub.status.idle": "2024-12-09T05:33:39.578480Z",
     "shell.execute_reply": "2024-12-09T05:33:39.577577Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.564120Z"
    },
    "id": "LazN3rY5xDiZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_torch_model(node_id):\n",
    "    model_path = f\"models/node_{node_id}.pth\"\n",
    "    model = torch.load(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_torch_model(model, node_id):\n",
    "    model_path = f\"models/node_{node_id}.pth\"\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "\n",
    "def save_model_param(model, node_id, round_number):\n",
    "    model_path = f\"models/node_{node_id}_round_{round_number}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Non-IID Distribution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "IMG_EXTENSIONS = (\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".png\",\n",
    "    \".ppm\",\n",
    "    \".bmp\",\n",
    "    \".pgm\",\n",
    "    \".tif\",\n",
    "    \".tiff\",\n",
    "    \".webp\",\n",
    ")\n",
    "\n",
    "\n",
    "def mkdirs(dirpath):\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "\n",
    "    if get_image_backend() == \"accimage\":\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "class CustomTensorDataset(data.TensorDataset):\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors) + (index,)\n",
    "\n",
    "\n",
    "class MNIST_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        mnist_dataobj = MNIST(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        data = mnist_dataobj.data\n",
    "        target = mnist_dataobj.targets\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        img = Image.fromarray(img.numpy(), mode=\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class FashionMNIST_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        mnist_dataobj = FashionMNIST(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        data = mnist_dataobj.data\n",
    "        target = mnist_dataobj.targets\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        img = Image.fromarray(img.numpy(), mode=\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class SVHN_custom(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "        if self.train is True:\n",
    "\n",
    "            svhn_dataobj = SVHN(\n",
    "                self.root, \"train\", self.transform, self.target_transform, self.download\n",
    "            )\n",
    "            data = svhn_dataobj.data\n",
    "            target = svhn_dataobj.labels\n",
    "        else:\n",
    "            svhn_dataobj = SVHN(\n",
    "                self.root, \"test\", self.transform, self.target_transform, self.download\n",
    "            )\n",
    "            data = svhn_dataobj.data\n",
    "            target = svhn_dataobj.labels\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# torchvision CelebA\n",
    "class CelebA_custom(VisionDataset):\n",
    "    \"\"\"`Large-scale CelebFaces Attributes (CelebA) Dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        split (string): One of {'train', 'valid', 'test', 'all'}.\n",
    "            Accordingly dataset is selected.\n",
    "        target_type (string or list, optional): Type of target to use, ``attr``, ``identity``, ``bbox``,\n",
    "            or ``landmarks``. Can also be a list to output a tuple with all specified target types.\n",
    "            The targets represent:\n",
    "                ``attr`` (np.array shape=(40,) dtype=int): binary (0, 1) labels for attributes\n",
    "                ``identity`` (int): label for each person (data points with the same identity are the same person)\n",
    "                ``bbox`` (np.array shape=(4,) dtype=int): bounding box (x, y, width, height)\n",
    "                ``landmarks`` (np.array shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x,\n",
    "                    righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y)\n",
    "            Defaults to ``attr``. If empty, ``None`` will be returned as target.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "\n",
    "    base_folder = \"celeba\"\n",
    "    # There currently does not appear to be a easy way to extract 7z in python (without introducing additional\n",
    "    # dependencies). The \"in-the-wild\" (not aligned+cropped) images are only in 7z, so they are not available\n",
    "    # right now.\n",
    "    file_list = [\n",
    "        # File ID                         MD5 Hash                            Filename\n",
    "        (\n",
    "            \"0B7EVK8r0v71pZjFTYXZWM3FlRnM\",\n",
    "            \"00d2c5bc6d35e252742224ab0c1e8fcb\",\n",
    "            \"img_align_celeba.zip\",\n",
    "        ),\n",
    "        # (\"0B7EVK8r0v71pbWNEUjJKdDQ3dGc\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_align_celeba_png.7z\"),\n",
    "        # (\"0B7EVK8r0v71peklHb0pGdDl6R28\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_celeba.7z\"),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pblRyaVFSWGxPY0U\",\n",
    "            \"75e246fa4810816ffd6ee81facbd244c\",\n",
    "            \"list_attr_celeba.txt\",\n",
    "        ),\n",
    "        (\n",
    "            \"1_ee_0u7vcNLOfNLegJRHmolfH5ICW-XS\",\n",
    "            \"32bd1bd63d3c78cd57e08160ec5ed1e2\",\n",
    "            \"identity_CelebA.txt\",\n",
    "        ),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pbThiMVRxWXZ4dU0\",\n",
    "            \"00566efa6fedff7a56946cd1c10f1c16\",\n",
    "            \"list_bbox_celeba.txt\",\n",
    "        ),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pd0FJY3Blby1HUTQ\",\n",
    "            \"cc24ecafdb5b50baae59b03474781f8c\",\n",
    "            \"list_landmarks_align_celeba.txt\",\n",
    "        ),\n",
    "        # (\"0B7EVK8r0v71pTzJIdlJWdHczRlU\", \"063ee6ddb681f96bc9ca28c6febb9d1a\", \"list_landmarks_celeba.txt\"),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pY0NSMzRuSXJEVkk\",\n",
    "            \"d32c9cbf5e040fd4025c592c306e6668\",\n",
    "            \"list_eval_partition.txt\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        split=\"train\",\n",
    "        target_type=\"attr\",\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        import pandas\n",
    "\n",
    "        super(CelebA_custom, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "        self.split = split\n",
    "        if isinstance(target_type, list):\n",
    "            self.target_type = target_type\n",
    "        else:\n",
    "            self.target_type = [target_type]\n",
    "\n",
    "        if not self.target_type and self.target_transform is not None:\n",
    "            raise RuntimeError(\"target_transform is specified but target_type is empty\")\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found or corrupted.\"\n",
    "                + \" You can use download=True to download it\"\n",
    "            )\n",
    "\n",
    "        split_map = {\n",
    "            \"train\": 0,\n",
    "            \"valid\": 1,\n",
    "            \"test\": 2,\n",
    "            \"all\": None,\n",
    "        }\n",
    "        split = split_map[split.lower()]\n",
    "\n",
    "        fn = partial(os.path.join, self.root, self.base_folder)\n",
    "        splits = pandas.read_csv(\n",
    "            fn(\"list_eval_partition.txt\"),\n",
    "            delim_whitespace=True,\n",
    "            header=None,\n",
    "            index_col=0,\n",
    "        )\n",
    "        identity = pandas.read_csv(\n",
    "            fn(\"identity_CelebA.txt\"), delim_whitespace=True, header=None, index_col=0\n",
    "        )\n",
    "        bbox = pandas.read_csv(\n",
    "            fn(\"list_bbox_celeba.txt\"), delim_whitespace=True, header=1, index_col=0\n",
    "        )\n",
    "        landmarks_align = pandas.read_csv(\n",
    "            fn(\"list_landmarks_align_celeba.txt\"), delim_whitespace=True, header=1\n",
    "        )\n",
    "        attr = pandas.read_csv(\n",
    "            fn(\"list_attr_celeba.txt\"), delim_whitespace=True, header=1\n",
    "        )\n",
    "\n",
    "        mask = slice(None) if split is None else (splits[1] == split)\n",
    "\n",
    "        self.filename = splits[mask].index.values\n",
    "        self.identity = torch.as_tensor(identity[mask].values)\n",
    "        self.bbox = torch.as_tensor(bbox[mask].values)\n",
    "        self.landmarks_align = torch.as_tensor(landmarks_align[mask].values)\n",
    "        self.attr = torch.as_tensor(attr[mask].values)\n",
    "        self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n",
    "        self.attr_names = list(attr.columns)\n",
    "        self.gender_index = self.attr_names.index(\"Male\")\n",
    "        self.dataidxs = dataidxs\n",
    "        if self.dataidxs is None:\n",
    "            self.target = self.attr[\n",
    "                :, self.gender_index : self.gender_index + 1\n",
    "            ].reshape(-1)\n",
    "        else:\n",
    "            self.target = self.attr[\n",
    "                self.dataidxs, self.gender_index : self.gender_index + 1\n",
    "            ].reshape(-1)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        for _, md5, filename in self.file_list:\n",
    "            fpath = os.path.join(self.root, self.base_folder, filename)\n",
    "            _, ext = os.path.splitext(filename)\n",
    "            # Allow original archive to be deleted (zip and 7z)\n",
    "            # Only need the extracted images\n",
    "            if ext not in [\".zip\", \".7z\"] and not check_integrity(fpath, md5):\n",
    "                return False\n",
    "\n",
    "        # Should check a hash of the images\n",
    "        return os.path.isdir(\n",
    "            os.path.join(self.root, self.base_folder, \"img_align_celeba\")\n",
    "        )\n",
    "\n",
    "    def download(self):\n",
    "        import zipfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print(\"Files already downloaded and verified\")\n",
    "            return\n",
    "\n",
    "        for file_id, md5, filename in self.file_list:\n",
    "            download_file_from_google_drive(\n",
    "                file_id, os.path.join(self.root, self.base_folder), filename, md5\n",
    "            )\n",
    "\n",
    "        with zipfile.ZipFile(\n",
    "            os.path.join(self.root, self.base_folder, \"img_align_celeba.zip\"), \"r\"\n",
    "        ) as f:\n",
    "            f.extractall(os.path.join(self.root, self.base_folder))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.dataidxs is None:\n",
    "            X = PIL.Image.open(\n",
    "                os.path.join(\n",
    "                    self.root,\n",
    "                    self.base_folder,\n",
    "                    \"img_align_celeba\",\n",
    "                    self.filename[index],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            target = []\n",
    "            for t in self.target_type:\n",
    "                if t == \"attr\":\n",
    "                    target.append(self.attr[index, self.gender_index])\n",
    "                elif t == \"identity\":\n",
    "                    target.append(self.identity[index, 0])\n",
    "                elif t == \"bbox\":\n",
    "                    target.append(self.bbox[index, :])\n",
    "                elif t == \"landmarks\":\n",
    "                    target.append(self.landmarks_align[index, :])\n",
    "                else:\n",
    "                    # TODO: refactor with utils.verify_str_arg\n",
    "                    raise ValueError('Target type \"{}\" is not recognized.'.format(t))\n",
    "        else:\n",
    "            X = PIL.Image.open(\n",
    "                os.path.join(\n",
    "                    self.root,\n",
    "                    self.base_folder,\n",
    "                    \"img_align_celeba\",\n",
    "                    self.filename[self.dataidxs[index]],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            target = []\n",
    "            for t in self.target_type:\n",
    "                if t == \"attr\":\n",
    "                    target.append(self.attr[self.dataidxs[index], self.gender_index])\n",
    "                elif t == \"identity\":\n",
    "                    target.append(self.identity[self.dataidxs[index], 0])\n",
    "                elif t == \"bbox\":\n",
    "                    target.append(self.bbox[self.dataidxs[index], :])\n",
    "                elif t == \"landmarks\":\n",
    "                    target.append(self.landmarks_align[self.dataidxs[index], :])\n",
    "                else:\n",
    "                    # TODO: refactor with utils.verify_str_arg\n",
    "                    raise ValueError('Target type \"{}\" is not recognized.'.format(t))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        # print(\"target[0]:\", target[0])\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "        # print(\"celeba target:\", target)\n",
    "        return X, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataidxs is None:\n",
    "            return len(self.attr)\n",
    "        else:\n",
    "            return len(self.dataidxs)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return \"\\n\".join(lines).format(**self.__dict__)\n",
    "\n",
    "\n",
    "class STL10_truncated(data.Dataset):\n",
    "    def __init__(self, root, dataidxs=None, split=\"train\", transform=None, target_transform=None, download=False):\n",
    "        \"\"\"\n",
    "        Custom STL10 dataset with support for data indexing.\n",
    "        Args:\n",
    "            root (str): Dataset root directory.\n",
    "            dataidxs (list, optional): Indices for data partitioning. Defaults to None.\n",
    "            split (str, optional): Dataset split ('train', 'test', 'unlabeled'). Defaults to 'train'.\n",
    "            transform (callable, optional): Transformations for the input data. Defaults to None.\n",
    "            target_transform (callable, optional): Transformations for the target labels. Defaults to None.\n",
    "            download (bool, optional): Whether to download the dataset. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "        stl10_dataobj = STL10(\n",
    "            self.root, split=self.split, transform=self.transform, target_transform=self.target_transform, download=self.download\n",
    "        )\n",
    "        data = stl10_dataobj.data\n",
    "        target = np.array(stl10_dataobj.labels)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is the class index.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "        img = Image.fromarray(img)  # Convert to PIL Image\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "class CIFAR10_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        cifar_dataobj = CIFAR10(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        data = cifar_dataobj.data\n",
    "        target = np.array(cifar_dataobj.targets)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def truncate_channel(self, index):\n",
    "        for i in range(index.shape[0]):\n",
    "            gs_index = index[i]\n",
    "            self.data[gs_index, :, :, 1] = 0.0\n",
    "            self.data[gs_index, :, :, 2] = 0.0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        # print(\"cifar10 img:\", img)\n",
    "        # print(\"cifar10 target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def gen_bar_updater() -> Callable[[int, int, int], None]:\n",
    "    pbar = tqdm(total=None)\n",
    "\n",
    "    def bar_update(count, block_size, total_size):\n",
    "        if pbar.total is None and total_size:\n",
    "            pbar.total = total_size\n",
    "        progress_bytes = count * block_size\n",
    "        pbar.update(progress_bytes - pbar.n)\n",
    "\n",
    "    return bar_update\n",
    "\n",
    "\n",
    "def download_url(\n",
    "    url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"Download a file from a url and place it in root.\n",
    "    Args:\n",
    "        url (str): URL to download file from\n",
    "        root (str): Directory to place downloaded file in\n",
    "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
    "        md5 (str, optional): MD5 checksum of the download. If None, do not check\n",
    "    \"\"\"\n",
    "    import urllib\n",
    "\n",
    "    root = os.path.expanduser(root)\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "    fpath = os.path.join(root, filename)\n",
    "\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "\n",
    "    # check if file is already present locally\n",
    "    if check_integrity(fpath, md5):\n",
    "        print(\"Using downloaded and verified file: \" + fpath)\n",
    "    else:  # download the file\n",
    "        try:\n",
    "            print(\"Downloading \" + url + \" to \" + fpath)\n",
    "            urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n",
    "        except (urllib.error.URLError, IOError) as e:  # type: ignore[attr-defined]\n",
    "            if url[:5] == \"https\":\n",
    "                url = url.replace(\"https:\", \"http:\")\n",
    "                print(\n",
    "                    \"Failed download. Trying https -> http instead.\"\n",
    "                    \" Downloading \" + url + \" to \" + fpath\n",
    "                )\n",
    "                urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n",
    "            else:\n",
    "                raise e\n",
    "        # check integrity of downloaded file\n",
    "        if not check_integrity(fpath, md5):\n",
    "            raise RuntimeError(\"File not found or corrupted.\")\n",
    "\n",
    "\n",
    "def _is_tarxz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar.xz\")\n",
    "\n",
    "\n",
    "def _is_tar(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar\")\n",
    "\n",
    "\n",
    "def _is_targz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar.gz\")\n",
    "\n",
    "\n",
    "def _is_tgz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tgz\")\n",
    "\n",
    "\n",
    "def _is_gzip(filename: str) -> bool:\n",
    "    return filename.endswith(\".gz\") and not filename.endswith(\".tar.gz\")\n",
    "\n",
    "\n",
    "def _is_zip(filename: str) -> bool:\n",
    "    return filename.endswith(\".zip\")\n",
    "\n",
    "\n",
    "def extract_archive(\n",
    "    from_path: str, to_path: Optional[str] = None, remove_finished: bool = False\n",
    ") -> None:\n",
    "    if to_path is None:\n",
    "        to_path = os.path.dirname(from_path)\n",
    "\n",
    "    if _is_tar(from_path):\n",
    "        with tarfile.open(from_path, \"r\") as tar:\n",
    "\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_targz(from_path) or _is_tgz(from_path):\n",
    "        with tarfile.open(from_path, \"r:gz\") as tar:\n",
    "\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_tarxz(from_path):\n",
    "        with tarfile.open(from_path, \"r:xz\") as tar:\n",
    "\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_gzip(from_path):\n",
    "        to_path = os.path.join(\n",
    "            to_path, os.path.splitext(os.path.basename(from_path))[0]\n",
    "        )\n",
    "        with open(to_path, \"wb\") as out_f, gzip.GzipFile(from_path) as zip_f:\n",
    "            out_f.write(zip_f.read())\n",
    "    elif _is_zip(from_path):\n",
    "        with zipfile.ZipFile(from_path, \"r\") as z:\n",
    "            z.extractall(to_path)\n",
    "    else:\n",
    "        raise ValueError(\"Extraction of {} not supported\".format(from_path))\n",
    "\n",
    "    if remove_finished:\n",
    "        os.remove(from_path)\n",
    "\n",
    "\n",
    "def download_and_extract_archive(\n",
    "    url: str,\n",
    "    download_root: str,\n",
    "    extract_root: Optional[str] = None,\n",
    "    filename: Optional[str] = None,\n",
    "    md5: Optional[str] = None,\n",
    "    remove_finished: bool = False,\n",
    ") -> None:\n",
    "    download_root = os.path.expanduser(download_root)\n",
    "    if extract_root is None:\n",
    "        extract_root = download_root\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "\n",
    "    download_url(url, download_root, filename, md5)\n",
    "\n",
    "    archive = os.path.join(download_root, filename)\n",
    "    print(\"Extracting {} to {}\".format(archive, extract_root))\n",
    "    extract_archive(archive, extract_root, remove_finished)\n",
    "\n",
    "\n",
    "class FEMNIST(MNIST):\n",
    "    \"\"\"\n",
    "    This dataset is derived from the Leaf repository\n",
    "    (https://github.com/TalwalkarLab/leaf) pre-processing of the Extended MNIST\n",
    "    dataset, grouping examples by writer. Details about Leaf were published in\n",
    "    \"LEAF: A Benchmark for Federated Settings\" https://arxiv.org/abs/1812.01097.\n",
    "    \"\"\"\n",
    "\n",
    "    resources = [\n",
    "        (\n",
    "            \"https://raw.githubusercontent.com/tao-shen/FEMNIST_pytorch/master/femnist.tar.gz\",\n",
    "            \"59c65cec646fc57fe92d27d83afdf0ed\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        super(MNIST, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "        self.train = train\n",
    "        self.dataidxs = dataidxs\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found.\" + \" You can use download=True to download it\"\n",
    "            )\n",
    "        if self.train:\n",
    "            data_file = self.training_file\n",
    "        else:\n",
    "            data_file = self.test_file\n",
    "\n",
    "        self.data, self.targets, self.users_index = torch.load(\n",
    "            os.path.join(self.processed_folder, data_file)\n",
    "        )\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            self.data = self.data[self.dataidxs]\n",
    "            self.targets = self.targets[self.dataidxs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        img = Image.fromarray(img.numpy(), mode=\"F\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the FEMNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        import shutil\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        mkdirs(self.raw_folder)\n",
    "        mkdirs(self.processed_folder)\n",
    "\n",
    "        # download files\n",
    "        for url, md5 in self.resources:\n",
    "            filename = url.rpartition(\"/\")[2]\n",
    "            download_and_extract_archive(\n",
    "                url, download_root=self.raw_folder, filename=filename, md5=md5\n",
    "            )\n",
    "\n",
    "        # process and save as torch files\n",
    "        print(\"Processing...\")\n",
    "        shutil.move(\n",
    "            os.path.join(self.raw_folder, self.training_file), self.processed_folder\n",
    "        )\n",
    "        shutil.move(\n",
    "            os.path.join(self.raw_folder, self.test_file), self.processed_folder\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_exists(self) -> bool:\n",
    "        return all(\n",
    "            check_integrity(\n",
    "                os.path.join(\n",
    "                    self.raw_folder,\n",
    "                    os.path.splitext(os.path.basename(url))[0]\n",
    "                    + os.path.splitext(os.path.basename(url))[1],\n",
    "                )\n",
    "            )\n",
    "            for url, _ in self.resources\n",
    "        )\n",
    "\n",
    "\n",
    "class Generated(MNIST):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        super(MNIST, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "        self.train = train\n",
    "        self.dataidxs = dataidxs\n",
    "\n",
    "        if self.train:\n",
    "            self.data = np.load(\"data/generated/X_train.npy\")\n",
    "            self.targets = np.load(\"data/generated/y_train.npy\")\n",
    "        else:\n",
    "            self.data = np.load(\"data/generated/X_test.npy\")\n",
    "            self.targets = np.load(\"data/generated/y_test.npy\")\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            self.data = self.data[self.dataidxs]\n",
    "            self.targets = self.targets[self.dataidxs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.data[index], self.targets[index]\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class genData(MNIST):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.data[index], self.targets[index]\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class CIFAR100_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        cifar_dataobj = CIFAR100(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        if torchvision.__version__ == \"0.2.1\":\n",
    "            if self.train:\n",
    "                data, target = cifar_dataobj.train_data, np.array(\n",
    "                    cifar_dataobj.train_labels\n",
    "                )\n",
    "            else:\n",
    "                data, target = cifar_dataobj.test_data, np.array(\n",
    "                    cifar_dataobj.test_labels\n",
    "                )\n",
    "        else:\n",
    "            data = cifar_dataobj.data\n",
    "            target = np.array(cifar_dataobj.targets)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "        img = Image.fromarray(img)\n",
    "        # print(\"cifar10 img:\", img)\n",
    "        # print(\"cifar10 target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class ImageFolder_custom(DatasetFolder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=None,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        imagefolder_obj = ImageFolder(self.root, self.transform, self.target_transform)\n",
    "        self.loader = imagefolder_obj.loader\n",
    "        if self.dataidxs is not None:\n",
    "            self.samples = np.array(imagefolder_obj.samples)[self.dataidxs]\n",
    "        else:\n",
    "            self.samples = np.array(imagefolder_obj.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.samples[index][0]\n",
    "        target = self.samples[index][1]\n",
    "        target = int(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataidxs is None:\n",
    "            return len(self.samples)\n",
    "        else:\n",
    "            return len(self.dataidxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdirs(dirpath):\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_mnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train_ds = MNIST_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    mnist_test_ds = MNIST_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = mnist_train_ds.data, mnist_train_ds.target\n",
    "    X_test, y_test = mnist_test_ds.data, mnist_test_ds.target\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_fmnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train_ds = FashionMNIST_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    mnist_test_ds = FashionMNIST_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = mnist_train_ds.data, mnist_train_ds.target\n",
    "    X_test, y_test = mnist_test_ds.data, mnist_test_ds.target\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_svhn_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    svhn_train_ds = SVHN_custom(datadir, train=True, download=True, transform=transform)\n",
    "    svhn_test_ds = SVHN_custom(datadir, train=False, download=True, transform=transform)\n",
    "    X_train, y_train = svhn_train_ds.data, svhn_train_ds.target\n",
    "    X_test, y_test = svhn_test_ds.data, svhn_test_ds.target\n",
    "    # X_train = X_train.data.numpy()\n",
    "    # y_train = y_train.data.numpy()\n",
    "    # X_test = X_test.data.numpy()\n",
    "    # y_test = y_test.data.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_cifar10_data(datadir):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    cifar10_train_ds = CIFAR10_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    cifar10_test_ds = CIFAR10_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = cifar10_train_ds.data, cifar10_train_ds.target\n",
    "    X_test, y_test = cifar10_test_ds.data, cifar10_test_ds.target\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_celeba_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    celeba_train_ds = CelebA_custom(\n",
    "        datadir, split=\"train\", target_type=\"attr\", download=True, transform=transform\n",
    "    )\n",
    "    celeba_test_ds = CelebA_custom(\n",
    "        datadir, split=\"test\", target_type=\"attr\", download=True, transform=transform\n",
    "    )\n",
    "    gender_index = celeba_train_ds.attr_names.index(\"Male\")\n",
    "    y_train = celeba_train_ds.attr[:, gender_index : gender_index + 1].reshape(-1)\n",
    "    y_test = celeba_test_ds.attr[:, gender_index : gender_index + 1].reshape(-1)\n",
    "    # y_train = y_train.numpy()\n",
    "    # y_test = y_test.numpy()\n",
    "    return (None, y_train, None, y_test)\n",
    "\n",
    "\n",
    "def load_femnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train_ds = FEMNIST(datadir, train=True, transform=transform, download=True)\n",
    "    mnist_test_ds = FEMNIST(datadir, train=False, transform=transform, download=True)\n",
    "    X_train, y_train, u_train = (\n",
    "        mnist_train_ds.data,\n",
    "        mnist_train_ds.targets,\n",
    "        mnist_train_ds.users_index,\n",
    "    )\n",
    "    X_test, y_test, u_test = (\n",
    "        mnist_test_ds.data,\n",
    "        mnist_test_ds.targets,\n",
    "        mnist_test_ds.users_index,\n",
    "    )\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    u_train = np.array(u_train)\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    u_test = np.array(u_test)\n",
    "    return (X_train, y_train, u_train, X_test, y_test, u_test)\n",
    "\n",
    "\n",
    "def load_cifar100_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    cifar100_train_ds = CIFAR100_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    cifar100_test_ds = CIFAR100_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = cifar100_train_ds.data, cifar100_train_ds.target\n",
    "    X_test, y_test = cifar100_test_ds.data, cifar100_test_ds.target\n",
    "    # y_train = y_train.numpy()\n",
    "    # y_test = y_test.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_tinyimagenet_data(datadir):\n",
    "    split = \"val\"\n",
    "    TinyImageNet(datadir, split=split)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    xray_train_ds = ImageFolder_custom(\n",
    "        datadir + \"tiny-imagenet-200/train/\", transform=transform\n",
    "    )\n",
    "    xray_test_ds = ImageFolder_custom(\n",
    "        datadir + \"tiny-imagenet-200/val/\", transform=transform\n",
    "    )\n",
    "    X_train, y_train = np.array([s[0] for s in xray_train_ds.samples]), np.array(\n",
    "        [int(s[1]) for s in xray_train_ds.samples]\n",
    "    )\n",
    "    X_test, y_test = np.array([s[0] for s in xray_test_ds.samples]), np.array(\n",
    "        [int(s[1]) for s in xray_test_ds.samples]\n",
    "    )\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "def load_stl10_data(datadir):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to VGG16 input size\n",
    "        transforms.RandomCrop(96, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    stl10_train_ds = STL10_truncated(datadir, split=\"train\", transform=transform_train, download=True)\n",
    "    stl10_test_ds = STL10_truncated(datadir, split=\"test\", transform=transform_test, download=True)\n",
    "\n",
    "    X_train, y_train = stl10_train_ds.data, stl10_train_ds.target\n",
    "    X_test, y_test = stl10_test_ds.data, stl10_test_ds.target\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def record_net_data_stats(y_train, net_dataidx_map, logdir):\n",
    "    net_cls_counts = {}\n",
    "    for net_i, dataidx in net_dataidx_map.items():\n",
    "        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True)\n",
    "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
    "        net_cls_counts[net_i] = tmp\n",
    "    logger.info(\"Data statistics: %s\" % str(net_cls_counts))\n",
    "    return net_cls_counts\n",
    "\n",
    "\n",
    "def partition_data(dataset, datadir, logdir, partition, n_parties, beta=0.4):\n",
    "    # Optional: set random seeds for reproducibility\n",
    "    # np.random.seed(2020)\n",
    "    # torch.manual_seed(2020)\n",
    "    # Initialize test data index map\n",
    "    test_dataidx_map = {}\n",
    "    # Load dataset\n",
    "    if dataset == \"mnist\":\n",
    "        X_train, y_train, X_test, y_test = load_mnist_data(datadir)\n",
    "    elif dataset == \"fmnist\":\n",
    "        X_train, y_train, X_test, y_test = load_fmnist_data(datadir)\n",
    "    elif dataset == \"cifar10\":\n",
    "        X_train, y_train, X_test, y_test = load_cifar10_data(datadir)\n",
    "    elif dataset == \"svhn\":\n",
    "        X_train, y_train, X_test, y_test = load_svhn_data(datadir)\n",
    "    elif dataset == \"celeba\":\n",
    "        X_train, y_train, X_test, y_test = load_celeba_data(datadir)\n",
    "    elif dataset == \"femnist\":\n",
    "        X_train, y_train, u_train, X_test, y_test, u_test = load_femnist_data(datadir)\n",
    "    elif dataset == \"cifar100\":\n",
    "        X_train, y_train, X_test, y_test = load_cifar100_data(datadir)\n",
    "    elif dataset == \"tinyimagenet\":\n",
    "        X_train, y_train, X_test, y_test = load_tinyimagenet_data(datadir)\n",
    "    elif dataset == \"stl10\":\n",
    "        X_train, y_train, X_test, y_test = load_stl10_data(datadir)\n",
    "    elif dataset == \"generated\":\n",
    "        # Code for generated dataset (omitted for brevity)\n",
    "        pass\n",
    "    # Add other datasets if needed\n",
    "    n_train = y_train.shape[0]\n",
    "    # Partition the data\n",
    "    if partition == \"homo\":\n",
    "        # Homogeneous data partition\n",
    "        idxs = np.random.permutation(n_train)\n",
    "        batch_idxs = np.array_split(idxs, n_parties)\n",
    "        net_dataidx_map = {i: batch_idxs[i] for i in range(n_parties)}\n",
    "    elif partition == \"noniid-labeldir\":\n",
    "        # Non-IID partition using Dirichlet distribution\n",
    "        # Code omitted for brevity\n",
    "        pass\n",
    "    elif partition.startswith(\"noniid-#label\") and partition[13:].isdigit():\n",
    "        # Non-IID partition where each client has a fixed number of labels\n",
    "        num = int(partition[13:])\n",
    "        if dataset in (\"celeba\", \"covtype\", \"a9a\", \"rcv1\", \"SUSY\"):\n",
    "            num = 1\n",
    "            K = 2\n",
    "        else:\n",
    "            if dataset == \"cifar100\":\n",
    "                K = 100\n",
    "            elif dataset == \"tinyimagenet\":\n",
    "                K = 200\n",
    "            else:\n",
    "                K = 10\n",
    "        if num == K:\n",
    "            # IID partition\n",
    "            net_dataidx_map = {\n",
    "                i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)\n",
    "            }\n",
    "            for i in range(K):\n",
    "                idx_k = np.where(y_train == i)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                split = np.array_split(idx_k, n_parties)\n",
    "                for j in range(n_parties):\n",
    "                    net_dataidx_map[j] = np.append(net_dataidx_map[j], split[j])\n",
    "        else:\n",
    "            times = [0 for _ in range(K)]\n",
    "            contain = []\n",
    "            for i in range(n_parties):\n",
    "                current = [i % K]\n",
    "                times[i % K] += 1\n",
    "                j = 1\n",
    "                while j < num:\n",
    "                    ind = random.randint(0, K - 1)\n",
    "                    if ind not in current:\n",
    "                        j += 1\n",
    "                        current.append(ind)\n",
    "                        times[ind] += 1\n",
    "                contain.append(current)\n",
    "            net_dataidx_map = {\n",
    "                i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)\n",
    "            }\n",
    "            test_dataidx_map = {\n",
    "                i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)\n",
    "            }\n",
    "            for i in range(K):\n",
    "                if times[i] > 0:\n",
    "                    idx_k = np.where(y_train == i)[0]\n",
    "                    idx_t = np.where(y_test == i)[0]\n",
    "                    np.random.shuffle(idx_k)\n",
    "                    np.random.shuffle(idx_t)\n",
    "                    split = np.array_split(idx_k, times[i])\n",
    "                    splitt = np.array_split(idx_t, times[i])\n",
    "                    ids = 0\n",
    "                    for j in range(n_parties):\n",
    "                        if i in contain[j]:\n",
    "                            net_dataidx_map[j] = np.append(\n",
    "                                net_dataidx_map[j], split[ids]\n",
    "                            )\n",
    "                            test_dataidx_map[j] = np.append(\n",
    "                                test_dataidx_map[j], splitt[ids]\n",
    "                            )\n",
    "                            ids += 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown partition method: {partition}\")\n",
    "    traindata_cls_counts = record_net_data_stats(y_train, net_dataidx_map, logdir)\n",
    "    return (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        net_dataidx_map,\n",
    "        test_dataidx_map,\n",
    "        traindata_cls_counts,\n",
    "    )\n",
    "\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=1.0, net_id=None, total=0):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.net_id = net_id\n",
    "        self.num = int(sqrt(total))\n",
    "        if self.num * self.num < total:\n",
    "            self.num = self.num + 1\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        if self.net_id is None:\n",
    "            return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "        else:\n",
    "            tmp = torch.randn(tensor.size())\n",
    "            filt = torch.zeros(tensor.size())\n",
    "            size = int(28 / self.num)\n",
    "            row = int(self.net_id / size)\n",
    "            col = self.net_id % size\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    filt[:, row * size + i, col * size + j] = 1\n",
    "            tmp = tmp * filt\n",
    "            return tensor + tmp * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"(mean={0}, std={1})\".format(\n",
    "            self.mean, self.std\n",
    "        )\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    dataset,\n",
    "    datadir,\n",
    "    train_bs,\n",
    "    test_bs,\n",
    "    dataidxs=None,\n",
    "    testidxs=None,\n",
    "    noise_level=0,\n",
    "    net_id=None,\n",
    "    total=0,\n",
    "):\n",
    "    if dataset in (\n",
    "        \"mnist\",\n",
    "        \"femnist\",\n",
    "        \"fmnist\",\n",
    "        \"cifar10\",\n",
    "        \"svhn\",\n",
    "        \"generated\",\n",
    "        \"covtype\",\n",
    "        \"a9a\",\n",
    "        \"rcv1\",\n",
    "        \"SUSY\",\n",
    "        \"cifar100\",\n",
    "        \"tinyimagenet\",\n",
    "        \"stl10\"\n",
    "    ):\n",
    "        if dataset == \"mnist\":\n",
    "            dl_obj = MNIST_truncated\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"femnist\":\n",
    "            dl_obj = FEMNIST\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"fmnist\":\n",
    "            dl_obj = FashionMNIST_truncated\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"svhn\":\n",
    "            dl_obj = SVHN_custom\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomRotation(10),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.ColorJitter(\n",
    "                        brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1\n",
    "                    ),\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"cifar10\":\n",
    "            print(\"in cifar10\")\n",
    "            dl_obj = CIFAR10_truncated\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    # transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Lambda(\n",
    "                        lambda x: F.pad(\n",
    "                            Variable(x.unsqueeze(0), requires_grad=False),\n",
    "                            (4, 4, 4, 4),\n",
    "                            mode=\"reflect\",\n",
    "                        ).data.squeeze()\n",
    "                    ),\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomCrop(32),\n",
    "                    transforms.ToTensor(),\n",
    "                    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            # data prep for test set\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"cifar100\":\n",
    "            print(\"in 100\")\n",
    "            dl_obj = CIFAR100_truncated\n",
    "            normalize = transforms.Normalize(\n",
    "                mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404],\n",
    "            )\n",
    "\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    # transforms.ToPILImage(),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ]\n",
    "            )\n",
    "            # data prep for test set\n",
    "            transform_test = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "        elif dataset == \"tinyimagenet\":\n",
    "            dl_obj = ImageFolder_custom\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(64, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4802, 0.4481, 0.3975), (0.2770, 0.2691, 0.2821)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.Resize((64, 64)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4802, 0.4481, 0.3975), (0.2770, 0.2691, 0.2821)), \n",
    "            ])\n",
    "        elif dataset == \"stl10\":\n",
    "            dl_obj = STL10_truncated\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomCrop(96, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        else:\n",
    "            dl_obj = Generated\n",
    "            transform_train = None\n",
    "            transform_test = None\n",
    "        if dataset == \"tinyimagenet\":\n",
    "            train_ds = dl_obj(\n",
    "                datadir + \"tiny-imagenet-200/train/\",\n",
    "                dataidxs=dataidxs,\n",
    "                transform=transform_train,\n",
    "            )\n",
    "            test_ds = dl_obj(\n",
    "                datadir + \"tiny-imagenet-200/val/\", \n",
    "                dataidxs=testidxs,\n",
    "                transform=transform_test\n",
    "            )\n",
    "        else:\n",
    "            print(\"dir\", datadir)\n",
    "            train_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=dataidxs,\n",
    "                train=True,\n",
    "                transform=transform_train,\n",
    "                download=True,\n",
    "            )\n",
    "            test_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=testidxs,\n",
    "                train=False,\n",
    "                transform=transform_test,\n",
    "                download=True,\n",
    "            )\n",
    "        train_dl = data.DataLoader(\n",
    "            dataset=train_ds, batch_size=train_bs, shuffle=True, drop_last=False\n",
    "        )\n",
    "        test_dl = data.DataLoader(\n",
    "            dataset=test_ds, batch_size=test_bs, shuffle=False, drop_last=False\n",
    "        )\n",
    "        print(train_ds, \"train ds\")\n",
    "    return train_dl, test_dl, train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(NUMBER_OF_CLIENTS):\n",
    "\n",
    "    (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        net_dataidx_map,\n",
    "        test_dataidx_map,\n",
    "        traindata_cls_counts,\n",
    "    ) = partition_data(\n",
    "        dataset=DATASET_TYPE,\n",
    "        datadir=\"./data/\",\n",
    "        logdir=\"./logs/\",\n",
    "        partition=PARTITION,\n",
    "        n_parties=10,\n",
    "    )\n",
    "    print(\"shapes\", X_train.shape, y_train.shape)\n",
    "    train_loaders = []\n",
    "    test_loaders = []\n",
    "    for client_id in range(NUMBER_OF_CLIENTS):\n",
    "\n",
    "        dataidxs = net_dataidx_map[client_id]\n",
    "        testidxs = test_dataidx_map[client_id]\n",
    "\n",
    "        train_dl_local, test_dl_local, train_ds_local, test_ds_local = get_dataloader(\n",
    "            dataset=DATASET_TYPE,\n",
    "            datadir=\"./data/\",\n",
    "            train_bs=128,\n",
    "            test_bs=128,\n",
    "            dataidxs=dataidxs,\n",
    "            testidxs=testidxs,\n",
    "        )\n",
    "        train_loaders.append(train_dl_local)\n",
    "        test_loaders.append(test_dl_local)\n",
    "\n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.579632Z",
     "iopub.status.busy": "2024-12-09T05:33:39.579393Z",
     "iopub.status.idle": "2024-12-09T05:33:39.590076Z",
     "shell.execute_reply": "2024-12-09T05:33:39.589219Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.579608Z"
    },
    "id": "-IvzdpYcxGZx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    train_loaders, test_loaders = get_loaders(10)\n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.591397Z",
     "iopub.status.busy": "2024-12-09T05:33:39.591067Z",
     "iopub.status.idle": "2024-12-09T05:33:47.647865Z",
     "shell.execute_reply": "2024-12-09T05:33:47.646680Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.591361Z"
    },
    "id": "ORJsNkg1xMY4",
    "outputId": "99812738-52e5-4abd-bafb-56edba8a171d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data statistics: {0: {np.int64(0): np.int64(5000), np.int64(2): np.int64(2500)}, 1: {np.int64(1): np.int64(1250), np.int64(9): np.int64(2500)}, 2: {np.int64(1): np.int64(1250), np.int64(2): np.int64(2500)}, 3: {np.int64(3): np.int64(2500), np.int64(4): np.int64(2500)}, 4: {np.int64(1): np.int64(1250), np.int64(4): np.int64(2500)}, 5: {np.int64(5): np.int64(5000), np.int64(7): np.int64(1667)}, 6: {np.int64(6): np.int64(2500), np.int64(7): np.int64(1667)}, 7: {np.int64(6): np.int64(2500), np.int64(7): np.int64(1666)}, 8: {np.int64(3): np.int64(2500), np.int64(8): np.int64(5000)}, 9: {np.int64(1): np.int64(1250), np.int64(9): np.int64(2500)}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes (50000, 32, 32, 3) (50000,)\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795c35633cb0> train ds\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795c21b5c3e0> train ds\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795d67e05580> train ds\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795d67e05ca0> train ds\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795d67e06210> train ds\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795d67e068a0> train ds\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795d67e07140> train ds\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795d67e07830> train ds\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795d67e07ef0> train ds\n",
      "in cifar10\n",
      "dir ./data/\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<__main__.CIFAR10_truncated object at 0x795d67e39c10> train ds\n"
     ]
    }
   ],
   "source": [
    "train_loaders, test_loaders = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Visualization</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:47:55.640148Z",
     "iopub.status.busy": "2024-12-09T05:47:55.639276Z",
     "iopub.status.idle": "2024-12-09T05:47:55.660170Z",
     "shell.execute_reply": "2024-12-09T05:47:55.659322Z",
     "shell.execute_reply.started": "2024-12-09T05:47:55.640096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    def __init__(self, train_loaders):\n",
    "        self.train_loaders = train_loaders\n",
    "\n",
    "    def count_classes(self):\n",
    "        class_counts = []\n",
    "        for loader in self.train_loaders:\n",
    "            counts = np.zeros(10, dtype=int)\n",
    "            for _, labels in loader:\n",
    "                for label in labels:\n",
    "                    counts[label] += 1\n",
    "            class_counts.append(counts)\n",
    "        return class_counts\n",
    "\n",
    "    def plot_class_distribution(\n",
    "        self,\n",
    "        DATASET_TYPE=\"Train\",\n",
    "    ):\n",
    "        class_counts = self.count_classes()\n",
    "        num_classes = NUMBER_OF_CLASSES\n",
    "        labels = [\n",
    "            \"airplane\",\n",
    "            \"automobile\",\n",
    "            \"bird\",\n",
    "            \"cat\",\n",
    "            \"deer\",\n",
    "            \"dog\",\n",
    "            \"frog\",\n",
    "            \"horse\",\n",
    "            \"ship\",\n",
    "            \"truck\",\n",
    "        ]\n",
    "        num_nodes = len(class_counts)\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        width = 0.6\n",
    "\n",
    "        counts = np.array(class_counts)\n",
    "        x = np.arange(num_nodes)\n",
    "\n",
    "        colors = plt.cm.tab10.colors\n",
    "\n",
    "        bottom = np.zeros(num_nodes)\n",
    "        for i in range(num_classes):\n",
    "            counts_per_class = counts[:, i]\n",
    "            ax.bar(\n",
    "                x,\n",
    "                counts_per_class,\n",
    "                width,\n",
    "                bottom=bottom,\n",
    "                label=labels[i],\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"white\",\n",
    "            )\n",
    "            bottom += counts_per_class\n",
    "        ax.set_xlabel(\"Nodes\")\n",
    "        ax.set_ylabel(\"Number of Samples\")\n",
    "        ax.set_title(f\"Distribution of {DATASET_TYPE} Classes Across Different Nodes\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([f\"{i+1}\" for i in range(num_nodes)], rotation=0)\n",
    "        ax.legend(\n",
    "            title=\"Classes\",\n",
    "            bbox_to_anchor=(1.05, 1),\n",
    "            loc=\"upper left\",\n",
    "            borderaxespad=0.0,\n",
    "            frameon=False,\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(right=0.75)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:48:37.189405Z",
     "iopub.status.busy": "2024-12-09T05:48:37.189041Z",
     "iopub.status.idle": "2024-12-09T05:48:40.830205Z",
     "shell.execute_reply": "2024-12-09T05:48:40.828992Z",
     "shell.execute_reply.started": "2024-12-09T05:48:37.189372Z"
    },
    "id": "KS4EpcfYxOK6",
    "outputId": "768f2820-33ea-44f5-8737-6f763f216c92",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizer(train_loaders).plot_class_distribution()\n",
    "# Visualizer(test_loaders).plot_class_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity matrix saved to clients_datasets_similarity_matrix.csv\n",
      "clients clustering based on their dataset:  {np.int64(0): [0, 2], np.int64(1): [1, 9], np.int64(2): [3, 4, 8], np.int64(3): [5, 6, 7]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_label_distribution(dataloader):\n",
    "    label_counts = np.zeros(NUMBER_OF_CLASSES)\n",
    "    for _, labels in dataloader:\n",
    "        for label in labels.numpy():\n",
    "            label_counts[label] += 1\n",
    "    label_distribution = label_counts / label_counts.sum()\n",
    "    return label_distribution\n",
    "\n",
    "\n",
    "def compute_similarity_matrix(distributions):\n",
    "    \"\"\"\n",
    "    Compute the pairwise similarity matrix for clients based on their label distributions.\n",
    "    Args:\n",
    "        distributions: List of label distributions (one per client).\n",
    "    Returns:\n",
    "        A similarity matrix.\n",
    "    \"\"\"\n",
    "    similarity_matrix = cosine_similarity(distributions)\n",
    "    return similarity_matrix\n",
    "\n",
    "def cluster_clients(similarity_matrix):\n",
    "    \"\"\"\n",
    "    Cluster clients based on their similarity matrix using Affinity Propagation.\n",
    "    Args:\n",
    "        similarity_matrix: Precomputed similarity matrix.\n",
    "    Returns:\n",
    "        Cluster labels for each client.\n",
    "    \"\"\"\n",
    "    clustering = AffinityPropagation(affinity='precomputed', random_state=42)\n",
    "    clustering.fit(similarity_matrix)\n",
    "    return clustering.labels_\n",
    "\n",
    "def group_clients_by_cluster(labels):\n",
    "    \"\"\"\n",
    "    Group clients based on their cluster labels.\n",
    "    Args:\n",
    "        labels: Cluster labels for each client.\n",
    "    Returns:\n",
    "        A dictionary where keys are cluster IDs and values are lists of client IDs.\n",
    "    \"\"\"\n",
    "    clusters = {}\n",
    "    for client_id, cluster_id in enumerate(labels):\n",
    "        if cluster_id not in clusters:\n",
    "            clusters[cluster_id] = []\n",
    "        clusters[cluster_id].append(client_id)\n",
    "    return clusters\n",
    "\n",
    "def save_similarity_matrix_to_csv(similarity_matrix, filename=\"similarity_matrix.csv\"):\n",
    "    \"\"\"\n",
    "    Save the similarity matrix to a CSV file.\n",
    "    Args:\n",
    "        similarity_matrix: The similarity matrix to save.\n",
    "        filename: The name of the CSV file.\n",
    "    \"\"\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header row\n",
    "        writer.writerow([\"Client\"] + [f\"Client_{i}\" for i in range(len(similarity_matrix))])\n",
    "        # Write each row of the similarity matrix\n",
    "        for i, row in enumerate(similarity_matrix):\n",
    "            writer.writerow([f\"Client_{i}\"] + row.tolist())\n",
    "    print(f\"Similarity matrix saved to {filename}\")\n",
    "    \n",
    "label_distributions = [calculate_label_distribution(loader) for loader in train_loaders]\n",
    "\n",
    "similarity_matrix = compute_similarity_matrix(label_distributions)\n",
    "\n",
    "save_similarity_matrix_to_csv(similarity_matrix, filename=\"clients_datasets_similarity_matrix.csv\")\n",
    "\n",
    "cluster_labels = cluster_clients(similarity_matrix)\n",
    "\n",
    "clusters = group_clients_by_cluster(cluster_labels)\n",
    "\n",
    "print(\"clients clustering based on their dataset: \", clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Executing</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:51.247202Z",
     "iopub.status.busy": "2024-12-09T05:33:51.246884Z",
     "iopub.status.idle": "2024-12-09T05:35:38.001063Z",
     "shell.execute_reply": "2024-12-09T05:35:37.999815Z",
     "shell.execute_reply.started": "2024-12-09T05:33:51.247162Z"
    },
    "id": "KTKWmDCUxXfO",
    "outputId": "147c0c4f-07b1-4428-a9a2-d43039f7249b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------Clustering step 0\n",
      "-------------in initial genertaio\n",
      "cluster [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "clientIDs [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "len_client_models(should be 10): 0\n",
      " ---in making new FL----cluster models len: 10 cluster IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "cid is: 0\n",
      "cid is: 1\n",
      "cid is: 2\n",
      "cid is: 3\n",
      "cid is: 4\n",
      "cid is: 5\n",
      "cid is: 6\n",
      "cid is: 7\n",
      "cid is: 8\n",
      "cid is: 9\n",
      "\n",
      "Round 1/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9161333333333334, test_acc:0.908\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.8733333333333333, test_acc:0.8293333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9608, test_acc:0.9586666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.851, test_acc:0.848\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9704, test_acc:0.9693333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9011549422528874, test_acc:0.8875562218890555\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9467242620590353, test_acc:0.9339735894357744\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9548727796447432, test_acc:0.9303721488595438\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9644, test_acc:0.9473333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.8706666666666667, test_acc:0.8386666666666667\n",
      "model numbers: 10\n",
      "time 0:03:17.765003\n",
      "global acc: 0.9051235293517707\n",
      "\n",
      "Round 2/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9154666666666667, test_acc:0.9106666666666666\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.8706666666666667, test_acc:0.828\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9538666666666666, test_acc:0.9586666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.8414, test_acc:0.836\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9653333333333334, test_acc:0.9453333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.8998050097495125, test_acc:0.8973013493253373\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9440844732421406, test_acc:0.9399759903961584\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9587133941430629, test_acc:0.9171668667466987\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9557333333333333, test_acc:0.9393333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.8669333333333333, test_acc:0.8666666666666667\n",
      "model numbers: 10\n",
      "time 0:06:33.792135\n",
      "global acc: 0.9039110873134861\n",
      "\n",
      "Round 3/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9154666666666667, test_acc:0.9093333333333333\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.8914666666666666, test_acc:0.8226666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9650666666666666, test_acc:0.9626666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.8556, test_acc:0.851\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9666666666666667, test_acc:0.98\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9001049947502625, test_acc:0.8845577211394303\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9462443004559635, test_acc:0.9483793517406963\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.957033125300048, test_acc:0.9339735894357744\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9572, test_acc:0.9386666666666666\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9045333333333333, test_acc:0.8186666666666667\n",
      "model numbers: 10\n",
      "time 0:09:51.596106\n",
      "global acc: 0.9049910662315901\n",
      "\n",
      "Round 4/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9268, test_acc:0.92\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9109333333333334, test_acc:0.888\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9754666666666667, test_acc:0.9706666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.8702, test_acc:0.859\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.976, test_acc:0.9746666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9101544922753863, test_acc:0.8958020989505248\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9565634749220062, test_acc:0.9531812725090036\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9584733557369179, test_acc:0.9375750300120048\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9624, test_acc:0.9613333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9093333333333333, test_acc:0.8573333333333333\n",
      "model numbers: 10\n",
      "time 0:13:04.483217\n",
      "global acc: 0.9217558401471534\n",
      "\n",
      "Round 5/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9282666666666667, test_acc:0.9313333333333333\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9349333333333333, test_acc:0.8453333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9709333333333333, test_acc:0.964\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.8966, test_acc:0.867\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9784, test_acc:0.964\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9145042747862607, test_acc:0.9070464767616192\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.949124070074394, test_acc:0.9411764705882353\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9579932789246279, test_acc:0.943577430972389\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9670666666666666, test_acc:0.958\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.928, test_acc:0.8893333333333333\n",
      "model numbers: 10\n",
      "time 0:16:10.230029\n",
      "global acc: 0.9210800378322244\n",
      "\n",
      "Round 6/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9406666666666667, test_acc:0.9166666666666666\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9394666666666667, test_acc:0.872\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9792, test_acc:0.9746666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.8956, test_acc:0.863\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9757333333333333, test_acc:0.976\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9248537573121344, test_acc:0.8913043478260869\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9683225341972642, test_acc:0.9627851140456183\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.969995199231877, test_acc:0.9543817527010804\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9750666666666666, test_acc:0.9626666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9373333333333334, test_acc:0.8946666666666667\n",
      "model numbers: 10\n",
      "time 0:19:41.958522\n",
      "global acc: 0.9268137881239454\n",
      "----------------------Info before clustering-------------\n",
      "model_len: 10\n",
      "Client IDS: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Model weights and sensitivity data for client #0 processed.\n",
      "Model weights and sensitivity data for client #1 processed.\n",
      "Model weights and sensitivity data for client #2 processed.\n",
      "Model weights and sensitivity data for client #3 processed.\n",
      "Model weights and sensitivity data for client #4 processed.\n",
      "Model weights and sensitivity data for client #5 processed.\n",
      "Model weights and sensitivity data for client #6 processed.\n",
      "Model weights and sensitivity data for client #7 processed.\n",
      "Model weights and sensitivity data for client #8 processed.\n",
      "Model weights and sensitivity data for client #9 processed.\n",
      "Distance matrix saved to distances_coordinate.csv\n",
      "cluster results:[0 1 2 3 2 3 3 3 3 1]\n",
      "new clustering: [[0], [1, 9], [2, 4], [3, 5, 6, 7, 8]]\n",
      "\n",
      "\n",
      "---------Clustering step 1\n",
      "-------------in initial genertaio\n",
      "cluster [0]\n",
      "clientIDs [0]\n",
      "len_client_models(should be 10): 10\n",
      " ---in making new FL----cluster models len: 1 cluster IDs: [0]\n",
      "cid is: 0\n",
      "\n",
      "Round 1/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9498666666666666, test_acc:0.9293333333333333\n",
      "model numbers: 1\n",
      "time 0:00:31.763195\n",
      "global acc: 0.9293333333333333\n",
      "\n",
      "Round 2/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9668, test_acc:0.928\n",
      "model numbers: 1\n",
      "time 0:01:04.347313\n",
      "global acc: 0.928\n",
      "\n",
      "Round 3/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9826666666666667, test_acc:0.94\n",
      "model numbers: 1\n",
      "time 0:01:35.076294\n",
      "global acc: 0.94\n",
      "\n",
      "Round 4/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9812, test_acc:0.9333333333333333\n",
      "model numbers: 1\n",
      "time 0:02:06.744434\n",
      "global acc: 0.9333333333333333\n",
      "\n",
      "Round 5/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9878666666666667, test_acc:0.922\n",
      "model numbers: 1\n",
      "time 0:02:35.174756\n",
      "global acc: 0.922\n",
      "\n",
      "Round 6/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9888, test_acc:0.938\n",
      "model numbers: 1\n",
      "time 0:03:02.464626\n",
      "global acc: 0.938\n",
      "-------------in initial genertaio\n",
      "cluster [1, 9]\n",
      "clientIDs [0, 1, 9]\n",
      "len_client_models(should be 10): 10\n",
      " ---in making new FL----cluster models len: 2 cluster IDs: [0, 1, 9]\n",
      "cid is: 1\n",
      "cid is: 9\n",
      "\n",
      "Round 1/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9408, test_acc:0.8813333333333333\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9370666666666667, test_acc:0.8853333333333333\n",
      "model numbers: 2\n",
      "time 0:00:28.025670\n",
      "global acc: 0.8833333333333333\n",
      "\n",
      "Round 2/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9706666666666667, test_acc:0.92\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9725333333333334, test_acc:0.9133333333333333\n",
      "model numbers: 2\n",
      "time 0:00:54.169657\n",
      "global acc: 0.9166666666666667\n",
      "\n",
      "Round 3/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.976, test_acc:0.92\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9765333333333334, test_acc:0.908\n",
      "model numbers: 2\n",
      "time 0:01:20.761144\n",
      "global acc: 0.914\n",
      "\n",
      "Round 4/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9770666666666666, test_acc:0.928\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9741333333333333, test_acc:0.908\n",
      "model numbers: 2\n",
      "time 0:01:47.410465\n",
      "global acc: 0.918\n",
      "\n",
      "Round 5/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9746666666666667, test_acc:0.9346666666666666\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9874666666666667, test_acc:0.9253333333333333\n",
      "model numbers: 2\n",
      "time 0:02:20.233577\n",
      "global acc: 0.9299999999999999\n",
      "\n",
      "Round 6/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9813333333333333, test_acc:0.936\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.984, test_acc:0.916\n",
      "model numbers: 2\n",
      "time 0:02:52.098317\n",
      "global acc: 0.926\n",
      "-------------in initial genertaio\n",
      "cluster [2, 4]\n",
      "clientIDs [0, 1, 9, 2, 4]\n",
      "len_client_models(should be 10): 10\n",
      " ---in making new FL----cluster models len: 2 cluster IDs: [0, 1, 9, 2, 4]\n",
      "cid is: 2\n",
      "cid is: 4\n",
      "\n",
      "Round 1/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9842666666666666, test_acc:0.9733333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9850666666666666, test_acc:0.9853333333333333\n",
      "model numbers: 2\n",
      "time 0:00:29.214734\n",
      "global acc: 0.9793333333333334\n",
      "\n",
      "Round 2/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9856, test_acc:0.9613333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9901333333333333, test_acc:0.9826666666666667\n",
      "model numbers: 2\n",
      "time 0:00:57.285375\n",
      "global acc: 0.972\n",
      "\n",
      "Round 3/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9896, test_acc:0.9733333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9917333333333334, test_acc:0.9853333333333333\n",
      "model numbers: 2\n",
      "time 0:01:24.478496\n",
      "global acc: 0.9793333333333334\n",
      "\n",
      "Round 4/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9850666666666666, test_acc:0.964\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9936, test_acc:0.9813333333333333\n",
      "model numbers: 2\n",
      "time 0:01:52.804351\n",
      "global acc: 0.9726666666666666\n",
      "\n",
      "Round 5/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9922666666666666, test_acc:0.976\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9941333333333333, test_acc:0.9813333333333333\n",
      "model numbers: 2\n",
      "time 0:02:19.287739\n",
      "global acc: 0.9786666666666666\n",
      "\n",
      "Round 6/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9909333333333333, test_acc:0.964\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9922666666666666, test_acc:0.984\n",
      "model numbers: 2\n",
      "time 0:02:51.081308\n",
      "global acc: 0.974\n",
      "-------------in initial genertaio\n",
      "cluster [3, 5, 6, 7, 8]\n",
      "clientIDs [0, 1, 9, 2, 4, 3, 5, 6, 7, 8]\n",
      "len_client_models(should be 10): 10\n",
      " ---in making new FL----cluster models len: 5 cluster IDs: [0, 1, 9, 2, 4, 3, 5, 6, 7, 8]\n",
      "cid is: 3\n",
      "cid is: 5\n",
      "cid is: 6\n",
      "cid is: 7\n",
      "cid is: 8\n",
      "\n",
      "Round 1/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9098, test_acc:0.897\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9389530523473826, test_acc:0.8988005997001499\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9721622270218383, test_acc:0.9495798319327731\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9735957753240518, test_acc:0.9663865546218487\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9758666666666667, test_acc:0.9546666666666667\n",
      "model numbers: 5\n",
      "time 0:01:49.613557\n",
      "global acc: 0.9332867305842877\n",
      "\n",
      "Round 2/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9238, test_acc:0.864\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9455527223638818, test_acc:0.9265367316341829\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9781617470602352, test_acc:0.9615846338535414\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9702352376380221, test_acc:0.957983193277311\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9769333333333333, test_acc:0.9633333333333334\n",
      "model numbers: 5\n",
      "time 0:03:40.186041\n",
      "global acc: 0.9346875784196736\n",
      "\n",
      "Round 3/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9464, test_acc:0.885\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9518524073796311, test_acc:0.9122938530734632\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.986321094312455, test_acc:0.963985594237695\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9875180028804609, test_acc:0.9663865546218487\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9809333333333333, test_acc:0.9606666666666667\n",
      "model numbers: 5\n",
      "time 0:05:27.670760\n",
      "global acc: 0.9376665337199348\n",
      "\n",
      "Round 4/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9438, test_acc:0.897\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9655017249137543, test_acc:0.9145427286356822\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9815214782817374, test_acc:0.9627851140456183\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.985837734037446, test_acc:0.9591836734693877\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9830666666666666, test_acc:0.956\n",
      "model numbers: 5\n",
      "time 0:07:15.480455\n",
      "global acc: 0.9379023032301376\n",
      "\n",
      "Round 5/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9466, test_acc:0.89\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9718014099295035, test_acc:0.9220389805097451\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9896808255339573, test_acc:0.9675870348139256\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9879980796927509, test_acc:0.957983193277311\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9854666666666667, test_acc:0.9646666666666667\n",
      "model numbers: 5\n",
      "time 0:09:10.572106\n",
      "global acc: 0.9404551750535297\n",
      "\n",
      "Round 6/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9352, test_acc:0.903\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9685015749212539, test_acc:0.9250374812593704\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.990160787137029, test_acc:0.9651860744297719\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9887181949111858, test_acc:0.957983193277311\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9850666666666666, test_acc:0.9613333333333334\n",
      "model numbers: 5\n",
      "time 0:10:58.341972\n",
      "global acc: 0.9425080164599573\n",
      "----------------------Info before clustering-------------\n",
      "model_len: 10\n",
      "Client IDS: [0, 1, 9, 2, 4, 3, 5, 6, 7, 8]\n",
      "Model weights and sensitivity data for client #0 processed.\n",
      "Model weights and sensitivity data for client #1 processed.\n",
      "Model weights and sensitivity data for client #9 processed.\n",
      "Model weights and sensitivity data for client #2 processed.\n",
      "Model weights and sensitivity data for client #4 processed.\n",
      "Model weights and sensitivity data for client #3 processed.\n",
      "Model weights and sensitivity data for client #5 processed.\n",
      "Model weights and sensitivity data for client #6 processed.\n",
      "Model weights and sensitivity data for client #7 processed.\n",
      "Model weights and sensitivity data for client #8 processed.\n",
      "Distance matrix saved to distances_coordinate.csv\n",
      "cluster results:[0 0 0 1 1 2 2 2 2 2]\n",
      "new clustering: [[0, 1, 9], [2, 4], [3, 5, 6, 7, 8]]\n",
      "\n",
      "\n",
      "---------Clustering step 2\n",
      "-------------in initial genertaio\n",
      "cluster [0, 1, 9]\n",
      "clientIDs [0, 1, 9]\n",
      "len_client_models(should be 10): 10\n",
      " ---in making new FL----cluster models len: 3 cluster IDs: [0, 1, 9]\n",
      "cid is: 0\n",
      "cid is: 1\n",
      "cid is: 9\n",
      "\n",
      "Round 1/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9908, test_acc:0.9426666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9885333333333334, test_acc:0.9253333333333333\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9890666666666666, test_acc:0.912\n",
      "model numbers: 3\n",
      "time 0:01:05.356784\n",
      "global acc: 0.9266666666666666\n",
      "\n",
      "Round 2/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9778666666666667, test_acc:0.9366666666666666\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9821333333333333, test_acc:0.912\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9842666666666666, test_acc:0.916\n",
      "model numbers: 3\n",
      "time 0:02:05.573702\n",
      "global acc: 0.9215555555555556\n",
      "\n",
      "Round 3/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9798666666666667, test_acc:0.9386666666666666\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9837333333333333, test_acc:0.9266666666666666\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9824, test_acc:0.912\n",
      "model numbers: 3\n",
      "time 0:03:06.643756\n",
      "global acc: 0.9257777777777778\n",
      "\n",
      "Round 4/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9814666666666667, test_acc:0.934\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9837333333333333, test_acc:0.9013333333333333\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9885333333333334, test_acc:0.9106666666666666\n",
      "model numbers: 3\n",
      "time 0:04:05.205167\n",
      "global acc: 0.9153333333333333\n",
      "\n",
      "Round 5/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9808, test_acc:0.9446666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9872, test_acc:0.9253333333333333\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.988, test_acc:0.9173333333333333\n",
      "model numbers: 3\n",
      "time 0:05:02.021968\n",
      "global acc: 0.9291111111111112\n",
      "\n",
      "Round 6/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.9821333333333333, test_acc:0.9353333333333333\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.9949333333333333, test_acc:0.9413333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.9877333333333334, test_acc:0.932\n",
      "model numbers: 3\n",
      "time 0:06:01.925766\n",
      "global acc: 0.9362222222222223\n",
      "-------------in initial genertaio\n",
      "cluster [2, 4]\n",
      "clientIDs [0, 1, 9, 2, 4]\n",
      "len_client_models(should be 10): 10\n",
      " ---in making new FL----cluster models len: 2 cluster IDs: [0, 1, 9, 2, 4]\n",
      "cid is: 2\n",
      "cid is: 4\n",
      "\n",
      "Round 1/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9896, test_acc:0.9746666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9944, test_acc:0.984\n",
      "model numbers: 2\n",
      "time 0:00:27.435962\n",
      "global acc: 0.9793333333333334\n",
      "\n",
      "Round 2/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9933333333333333, test_acc:0.9706666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9962666666666666, test_acc:0.9866666666666667\n",
      "model numbers: 2\n",
      "time 0:00:59.171334\n",
      "global acc: 0.9786666666666667\n",
      "\n",
      "Round 3/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9949333333333333, test_acc:0.9773333333333334\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9976, test_acc:0.988\n",
      "model numbers: 2\n",
      "time 0:01:28.066585\n",
      "global acc: 0.9826666666666667\n",
      "\n",
      "Round 4/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9864, test_acc:0.9706666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9933333333333333, test_acc:0.9853333333333333\n",
      "model numbers: 2\n",
      "time 0:01:56.406322\n",
      "global acc: 0.978\n",
      "\n",
      "Round 5/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9957333333333334, test_acc:0.9786666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9954666666666667, test_acc:0.98\n",
      "model numbers: 2\n",
      "time 0:02:25.954574\n",
      "global acc: 0.9793333333333334\n",
      "\n",
      "Round 6/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.9930666666666667, test_acc:0.9786666666666667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.9986666666666667, test_acc:0.9853333333333333\n",
      "model numbers: 2\n",
      "time 0:02:53.069367\n",
      "global acc: 0.982\n",
      "-------------in initial genertaio\n",
      "cluster [3, 5, 6, 7, 8]\n",
      "clientIDs [0, 1, 9, 2, 4, 3, 5, 6, 7, 8]\n",
      "len_client_models(should be 10): 10\n",
      " ---in making new FL----cluster models len: 5 cluster IDs: [0, 1, 9, 2, 4, 3, 5, 6, 7, 8]\n",
      "cid is: 3\n",
      "cid is: 5\n",
      "cid is: 6\n",
      "cid is: 7\n",
      "cid is: 8\n",
      "\n",
      "Round 1/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9618, test_acc:0.894\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9727013649317534, test_acc:0.9152923538230885\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9916006719462443, test_acc:0.9699879951980792\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9942390782525204, test_acc:0.9615846338535414\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9877333333333334, test_acc:0.9646666666666667\n",
      "model numbers: 5\n",
      "time 0:01:43.029588\n",
      "global acc: 0.9411063299082751\n",
      "\n",
      "Round 2/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9604, test_acc:0.911\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9670016499175041, test_acc:0.9212893553223388\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9880009599232061, test_acc:0.9591836734693877\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9920787325972156, test_acc:0.9663865546218487\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9877333333333334, test_acc:0.9586666666666667\n",
      "model numbers: 5\n",
      "time 0:03:25.406389\n",
      "global acc: 0.9433052500160484\n",
      "\n",
      "Round 3/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9682, test_acc:0.903\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9752512374381281, test_acc:0.9205397301349325\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.988240940724742, test_acc:0.9759903961584634\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.9903984637542007, test_acc:0.9699879951980792\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.9850666666666666, test_acc:0.9546666666666667\n",
      "model numbers: 5\n",
      "time 0:05:05.057983\n",
      "global acc: 0.9448369576316283\n",
      "\n",
      "Round 4/6\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.9666, test_acc:0.889\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.9784010799460027, test_acc:0.9325337331334332\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.9920806335493161, test_acc:0.9591836734693877\n"
     ]
    }
   ],
   "source": [
    "clusters=[]\n",
    "initial = [i for i in range(NUMBER_OF_CLIENTS)]\n",
    "clusters.append(initial)\n",
    "\n",
    "\n",
    "def generate_initial_models(step,cluster,client_ids,client_Models):\n",
    "    print(\"-------------in initial genertaio\")\n",
    "    print(\"cluster\", cluster)\n",
    "    print(\"clientIDs\", client_ids)\n",
    "    print(\"len_client_models(should be 10):\",len(client_Models))\n",
    "    list1=[]\n",
    "\n",
    "    if step==0:\n",
    "        for member in range(len(cluster)):\n",
    "            list1.append(Net())\n",
    "    else:\n",
    "        for index in cluster:\n",
    "            list1.append(client_Models[client_ids.index(index)])\n",
    "    return list1\n",
    "\n",
    "\n",
    "client_Models=[]\n",
    "client_copy_models = []\n",
    "\n",
    "for step in range(CLUSTERING_PERIOD):\n",
    "    client_copy_models=copy.deepcopy(client_Models)\n",
    "    client_Models=[]\n",
    "    print(\"\\n\\n---------Clustering step\", step)\n",
    "    FL_list=[]\n",
    "    client_ids=[]\n",
    "    for cluster in clusters:\n",
    "        for Id in cluster:\n",
    "            client_ids.append(Id)\n",
    "        cluster_initial_models=generate_initial_models(step,cluster,client_ids,client_copy_models)\n",
    "        print(\" ---in making new FL----cluster models len:\", len(cluster_initial_models),\"cluster IDs:\", client_ids)\n",
    "        f = FL(cluster,cluster_initial_models,FEDERATED_LEARNING_ROUNDS, train_loaders, test_loaders, SENSITIVITY_PERCENTAGE)\n",
    "        FL_list.append(f)\n",
    "        for member in f.client_obj_list:\n",
    "            client_Models.append(member.net)\n",
    "        for cid in client_ids:\n",
    "            save_torch_model(client_Models[client_ids.index(cid)], cid)\n",
    "            # save_model_param(client_Models[client_ids.index(cid)], cid, step)\n",
    "    \n",
    "    print(\"----------------------Info before clustering-------------\")\n",
    "    print(\"model_len:\", len(client_Models))\n",
    "    print(\"Client IDS:\",client_ids )\n",
    "    start_cluster_time = datetime.now()\n",
    "    clusters = Clustering(client_ids, train_loaders, SENSITIVITY_PERCENTAGE).Clusters\n",
    "    end_cluster_time = datetime.now()\n",
    "    exe_cluster_time = end_cluster_time - start_cluster_time\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"\\n Exe Cluster Time: {exe_cluster_time}\")\n",
    "    print(\"new clustering:\",clusters)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6072062,
     "sourceId": 9887519,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
