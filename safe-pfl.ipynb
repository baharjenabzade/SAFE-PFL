{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Install Pacakges</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:45:45.266838Z",
     "iopub.status.busy": "2024-12-09T05:45:45.266370Z",
     "iopub.status.idle": "2024-12-09T05:46:14.685176Z",
     "shell.execute_reply": "2024-12-09T05:46:14.683826Z",
     "shell.execute_reply.started": "2024-12-09T05:45:45.266799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip uninstall nvidia-ml-py pynvml -y --quiet\n",
    "# !pip install --force-reinstall nvidia-ml-py --quiet\n",
    "# !pip install datasets lxml lightning zeus-ml --quiet\n",
    "!pip install datasets lxml\n",
    "!pip install tikzplotlib  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Import Libraries</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:48:24.941604Z",
     "iopub.status.busy": "2024-12-09T05:48:24.941087Z",
     "iopub.status.idle": "2024-12-09T05:48:25.126240Z",
     "shell.execute_reply": "2024-12-09T05:48:25.125344Z",
     "shell.execute_reply.started": "2024-12-09T05:48:24.941557Z"
    },
    "id": "IWgcTDs4vXBk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from scipy import spatial\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "import torch\n",
    "import ast\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    ToTensor,\n",
    "    Normalize,\n",
    "    Compose,\n",
    "    Resize,\n",
    "    Grayscale,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomRotation,\n",
    ")\n",
    "import math\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
    "from torchvision.datasets import CIFAR10, MNIST, FashionMNIST\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from numba import cuda\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import copy\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import MDS\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import tikzplotlib\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Garbage Collection</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "def print_gpu_memory():\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "    \n",
    "print_gpu_memory()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cuda.select_device(0) \n",
    "cuda.close()\n",
    "\n",
    "print(\"after memory cleaning\")\n",
    "print_gpu_memory()\n",
    "\n",
    "#----------- manually clear memory in case of any error\n",
    "#!sudo fuser -v /dev/nvidia* or nvidia-smi\n",
    "# remove all python process ids from gpu\n",
    "#!sudo kill -9 PID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Make Directories</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:34.456780Z",
     "iopub.status.busy": "2024-12-09T05:33:34.456325Z",
     "iopub.status.idle": "2024-12-09T05:33:38.523961Z",
     "shell.execute_reply": "2024-12-09T05:33:38.522694Z",
     "shell.execute_reply.started": "2024-12-09T05:33:34.456716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "!mkdir models/before_agg\n",
    "!mkdir sensitive_id\n",
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:38.525906Z",
     "iopub.status.busy": "2024-12-09T05:33:38.525588Z",
     "iopub.status.idle": "2024-12-09T05:33:38.531683Z",
     "shell.execute_reply": "2024-12-09T05:33:38.530748Z",
     "shell.execute_reply.started": "2024-12-09T05:33:38.525874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "log_path = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S_\")\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "log_file = log_path + \".log\"\n",
    "\n",
    "os.mkdir(log_path)\n",
    "\n",
    "open(log_file, \"a\").close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Configs</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:38.533041Z",
     "iopub.status.busy": "2024-12-09T05:33:38.532778Z",
     "iopub.status.idle": "2024-12-09T05:33:38.790687Z",
     "shell.execute_reply": "2024-12-09T05:33:38.789843Z",
     "shell.execute_reply.started": "2024-12-09T05:33:38.533002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "pl.seed_everything(seed)\n",
    "sns.set_theme(style=\"darkgrid\", font_scale=1.5, font=\"SimHei\", rc={\"axes.unicode_minus\":False})\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "num_shards_per_class = 20\n",
    "num_clients = 10\n",
    "NUMBER_OF_CLASSES=200 # i parsed it to Net class\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Clustering_method=\"Mask\"# Weight, Mask\n",
    "Clustering_period = 5\n",
    "cluster_number=3\n",
    "FL_rounds = 6\n",
    "Sensitivity_percentage = 0.1\n",
    "# Sensitivity_percentage = 1\n",
    "\n",
    "dataset_type=\"tinyimagenet\" #cifar, mnist, tinyimagenet\n",
    "model_type=\"alexnet\" #resnet18, mobilenet, vgg16, cnn, alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Model Network</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.351995Z",
     "iopub.status.busy": "2024-12-09T05:33:39.350967Z",
     "iopub.status.idle": "2024-12-09T05:33:39.367653Z",
     "shell.execute_reply": "2024-12-09T05:33:39.366717Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.351954Z"
    },
    "id": "evEmrviBwIoH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        if model_type == \"resnet18\":\n",
    "            self.resnet18 = models.resnet18(pretrained=False)\n",
    "            if dataset_type == \"mnist\":\n",
    "                self.resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "            self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, NUMBER_OF_CLASSES)\n",
    "\n",
    "        elif model_type == \"cnn\":\n",
    "            self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "            self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        elif model_type == \"mobilenet\":\n",
    "            self.mobilenet = models.mobilenet_v2(pretrained=False)\n",
    "            self.mobilenet.classifier[1] = nn.Linear(self.mobilenet.last_channel, NUMBER_OF_CLASSES)\n",
    "\n",
    "        elif model_type == \"vgg16\":\n",
    "            self.vgg16 = models.vgg16(pretrained=False)\n",
    "            self.vgg16.classifier[6] = nn.Linear(self.vgg16.classifier[6].in_features, NUMBER_OF_CLASSES)\n",
    "\n",
    "        elif model_type == \"alexnet\":\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(256 * 6 * 6, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(4096, NUMBER_OF_CLASSES),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = None\n",
    "\n",
    "        if model_type == \"resnet18\":\n",
    "            out = self.resnet18(x)\n",
    "        elif model_type == \"cnn\":\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = x.view(x.size(0), 16 * 5 * 5)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            out = x\n",
    "\n",
    "            out = x\n",
    "        elif model_type == \"mobilenet\":\n",
    "            out = self.mobilenet(x)\n",
    "            \n",
    "        elif model_type == \"vgg16\":\n",
    "            out = self.vgg16(x)\n",
    "        \n",
    "        elif model_type == \"alexnet\":\n",
    "            x = self.features(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.classifier(x)\n",
    "            out = x\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Learning</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.370196Z",
     "iopub.status.busy": "2024-12-09T05:33:39.369557Z",
     "iopub.status.idle": "2024-12-09T05:33:39.387952Z",
     "shell.execute_reply": "2024-12-09T05:33:39.386804Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.370159Z"
    },
    "id": "cCM4LUpzwTXE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(loader, model):\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in loader:\n",
    "\n",
    "            images, labels = data\n",
    "\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(net, node_id, trainloader, epochs: int):\n",
    "\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    net.train()\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(images)\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "\n",
    "              outputs, aux1, aux2 = outputs\n",
    "\n",
    "              loss = criterion(outputs, labels) + 0.3 * criterion(aux1, labels) + 0.3 * criterion(aux2, labels)\n",
    "\n",
    "            else:\n",
    "\n",
    "              loss = criterion(outputs, labels)\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "\n",
    "            epoch_loss += loss\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            ## outputs.data\n",
    "\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "    loss /= len(trainloader.dataset)\n",
    "\n",
    "    acc = correct / total\n",
    "\n",
    "    model_path = f\"models/node_{node_id}.pth\"\n",
    "\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    return acc, loss\n",
    "\n",
    "    \n",
    "\n",
    "def test(net, testloader):\n",
    "\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for images, labels in testloader:\n",
    "\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = net(images)\n",
    "\n",
    "            ## For googlenet model\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "\n",
    "              outputs, _, _ = outputs\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            loss += criterion(outputs, labels).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    loss /= len(testloader.dataset)\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # print(f\" test loss {loss}, accuracy {accuracy}\")\n",
    "\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Client</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.390658Z",
     "iopub.status.busy": "2024-12-09T05:33:39.390325Z",
     "iopub.status.idle": "2024-12-09T05:33:39.416416Z",
     "shell.execute_reply": "2024-12-09T05:33:39.414449Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.390620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Client:\n",
    "\n",
    "    def __init__(self, net, node_id, trainloader, testloader):\n",
    "\n",
    "        self.net = net.to(DEVICE)\n",
    "\n",
    "        self.trainloader = trainloader\n",
    "\n",
    "        self.testloader = testloader\n",
    "\n",
    "        self.node_id = node_id\n",
    "\n",
    "        self.train_acc, self.test_acc = 0.0, 0.0\n",
    "\n",
    "        self.global_net = Net().to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "    # def train_test(self):\n",
    "\n",
    "    #     self.train_acc,_=\n",
    "\n",
    "    #     self.test_acc\n",
    "\n",
    "    def set_bias(self, pref, bias):\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "        self.pref = pref\n",
    "\n",
    "    def set_shard(self, shard):\n",
    "\n",
    "        self.shard = shard\n",
    "\n",
    "    def get_global_net(self):\n",
    "\n",
    "        return self.global_net\n",
    "\n",
    "    def setting_parameters(self, parameters: List[np.ndarray]):\n",
    "\n",
    "        params_dict = zip(self.net.state_dict().items(), parameters)\n",
    "\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v).to(DEVICE) for k, v in params_dict})\n",
    "\n",
    "        self.net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def getting_parameters(self) -> List[np.ndarray]:\n",
    "\n",
    "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
    "\n",
    "    # def get_parameters(self):\n",
    "\n",
    "    # return getting_parameters()\n",
    "\n",
    "    def fit(self, parameters):\n",
    "\n",
    "        self.setting_parameters(parameters)\n",
    "\n",
    "        train(self.net, self.node_id, self.trainloader, epochs=Round_Epochs)\n",
    "\n",
    "        return self.getting_parameters(), len(self.trainloader), {}\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters):\n",
    "\n",
    "        self.setting_parameters(parameters)\n",
    "\n",
    "        loss, accuracy = test(self.net, self.testloader)\n",
    "\n",
    "        return float(loss), len(self.testloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "\n",
    "    def Train_test_and_return_acc(self):\n",
    "\n",
    "        # monitor.begin_window(\"training\")\n",
    "        self.train_acc, _ = train(self.net, self.node_id, self.trainloader, 1)\n",
    "        # measurement = monitor.end_window(\"training\")\n",
    "        # print(f\"Entire training: {measurement.time} s, {measurement.total_energy} J for node: {self.node_id}\")\n",
    "\n",
    "        # monitor.begin_window(\"testing\")\n",
    "        self.test_acc, _ = test(self.net, self.testloader)\n",
    "        # measurement = monitor.end_window(\"testing\")\n",
    "        # print(f\"Entire testing: {measurement.time} s, {measurement.total_energy} J for node: {self.node_id}\")\n",
    "\n",
    "        return self.train_acc, self.test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Server</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.420149Z",
     "iopub.status.busy": "2024-12-09T05:33:39.418276Z",
     "iopub.status.idle": "2024-12-09T05:33:39.455212Z",
     "shell.execute_reply": "2024-12-09T05:33:39.453215Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.420094Z"
    },
    "id": "0SW7jKZNwbhJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def divide_nested_list(nested_list, divisor):\n",
    "\n",
    "    for i in range(len(nested_list)):\n",
    "\n",
    "        if isinstance(nested_list[i], list):\n",
    "\n",
    "            divide_nested_list(nested_list[i], divisor)\n",
    "\n",
    "        else:\n",
    "\n",
    "            nested_list[i] /= divisor\n",
    "\n",
    "    return nested_list\n",
    "\n",
    "\n",
    "\n",
    "def zero_nested_list(nested_list):\n",
    "\n",
    "    for i in range(len(nested_list)):\n",
    "\n",
    "        if isinstance(nested_list[i], list):\n",
    "\n",
    "            zero_nested_list(nested_list[i])\n",
    "\n",
    "        else:\n",
    "\n",
    "            nested_list[i] = 0\n",
    "\n",
    "    return nested_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Server:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.models = []\n",
    "\n",
    "\n",
    "\n",
    "    def append_model(self, model: nn.Module):\n",
    "\n",
    "        if not isinstance(model, nn.Module):\n",
    "\n",
    "            raise TypeError(\"Only instances of nn.Module can be appended\")\n",
    "\n",
    "        self.models.append(model)\n",
    "\n",
    "\n",
    "\n",
    "    def aggregate(self):\n",
    "\n",
    "        if not self.models:\n",
    "\n",
    "            raise ValueError(\"No models added to the server.\")\n",
    "\n",
    "        print(\"model numbers:\", len(self.models))\n",
    "\n",
    "\n",
    "\n",
    "        device = next(self.models[0].parameters()).device\n",
    "\n",
    "\n",
    "\n",
    "        for model in self.models:\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        avg_model = Net().to(device)\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for param_name, avg_param in avg_model.named_parameters():\n",
    "\n",
    "                temp = torch.zeros_like(avg_param)\n",
    "\n",
    "                for model in self.models:\n",
    "\n",
    "                    model_param = dict(model.named_parameters())[param_name]\n",
    "\n",
    "                    temp += model_param.data\n",
    "\n",
    "                avg_param.copy_(temp / len(self.models))\n",
    "\n",
    "\n",
    "\n",
    "        return avg_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Clustering</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.457583Z",
     "iopub.status.busy": "2024-12-09T05:33:39.457181Z",
     "iopub.status.idle": "2024-12-09T05:33:39.544421Z",
     "shell.execute_reply": "2024-12-09T05:33:39.543934Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.457534Z"
    },
    "id": "asSZtfDAwmrd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_num_cluster(clusters):\n",
    "\n",
    "    num_cluster = []\n",
    "\n",
    "    for item in clusters:\n",
    "\n",
    "        if item not in num_cluster:\n",
    "\n",
    "            num_cluster.append(item)\n",
    "\n",
    "    return len(num_cluster)\n",
    "\n",
    "\n",
    "\n",
    "class Clustering:\n",
    "\n",
    "    def __init__(self, clients, trainLoaders, percentage, Cluster_number):\n",
    "\n",
    "        # self.models=models\n",
    "\n",
    "        self.clients = clients\n",
    "\n",
    "        self.num_nodes = len(clients)\n",
    "\n",
    "        self.percentage = percentage\n",
    "\n",
    "        self.Mask_Number = 0\n",
    "\n",
    "        self.maskIds = []\n",
    "\n",
    "        self.grads = []\n",
    "\n",
    "\n",
    "\n",
    "        # self.sensitivity_values=[self.calculate_sensitivity(models[i],trainLoaders[i]) for i in range (len(models))]\n",
    "\n",
    "\n",
    "\n",
    "        self.load_and_calculate_sensitivity(trainLoaders)\n",
    "\n",
    "\n",
    "\n",
    "        # df = pd.read_csv(\"clients.csv\")\n",
    "\n",
    "        # self.maskIds=[self.get_maskIds(self.sensitivity_values[i]) for i in range (self.num_nodes)]\n",
    "\n",
    "        # = df[\"mask_IDs\"]\n",
    "\n",
    "        # print(\"Mask IDs:\",self.maskIds)\n",
    "\n",
    "        self.Cluster_number = Cluster_number\n",
    "\n",
    "        self.distances = self.calculate_distance()\n",
    "\n",
    "        # print(\"Mask_Number:\",self.Mask_Number)\n",
    "\n",
    "        # print(\"dist:\",self.distances)\n",
    "\n",
    "        self.Clusters = self.make_clusters()\n",
    "\n",
    "    def assign_save_ids_to_weights(self, model):\n",
    "\n",
    "        weight_id_map = {}\n",
    "\n",
    "        weight_id = 0\n",
    "\n",
    "        for name, parameter in model.named_parameters():\n",
    "\n",
    "            # bias anabled\n",
    "            # if \"bias\" not in name and parameter.requires_grad:\n",
    "\n",
    "            weight_id_map[name] = {}\n",
    "\n",
    "            num_weights = parameter.numel()\n",
    "\n",
    "            for i in range(num_weights):\n",
    "\n",
    "                weight_id_map[name][i] = weight_id\n",
    "\n",
    "                weight_id += 1\n",
    "\n",
    "        filename = \"weight_to_id.csv\"\n",
    "\n",
    "        if not os.path.exists(filename):\n",
    "\n",
    "            with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "\n",
    "                writer = csv.writer(csvfile)\n",
    "\n",
    "                writer.writerow([\"Layer\", \"Weight Index\", \"Weight ID\"])\n",
    "\n",
    "                for layer_name, indices in weight_id_map.items():\n",
    "\n",
    "                    for index, weight_id in indices.items():\n",
    "\n",
    "                        writer.writerow([layer_name, index, weight_id])\n",
    "\n",
    "        return weight_id_map\n",
    "\n",
    "    \n",
    "\n",
    "    def load_and_calculate_sensitivity(self, trainLoaders, log_dir=\"sensitive_id\"):\n",
    "        \"\"\"\n",
    "        Calculate sensitivity and save unique Mask IDs for each client to separate CSV files.\n",
    "        \"\"\"\n",
    "        # monitor.begin_window(\"load_and_calculate_sensitivity\")\n",
    "        \n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        for cid in self.clients:\n",
    "            log_file = os.path.join(log_dir, f\"client_{cid}_weights_log.csv\")\n",
    "            \n",
    "            with open(log_file, mode=\"w\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Mask IDs\"])  # Single header row\n",
    "                \n",
    "                model = load_torch_model(cid).to(DEVICE)\n",
    "                \n",
    "                # monitor.begin_window(\"calculate_sensitivity\")\n",
    "                sensitivity_value = self.calculate_sensitivity(model, trainLoaders[int(cid)])\n",
    "                # measurement = monitor.end_window(\"calculate_sensitivity\")\n",
    "                # print(f\"Entire calculate sensitivity for client #{cid}: {measurement.time} s, {measurement.total_energy} J\")\n",
    "                \n",
    "                weight_id_map = self.assign_save_ids_to_weights(load_torch_model(0).to(DEVICE))\n",
    "                mask_ID, weights = self.get_maskIds(sensitivity_value, weight_id_map)\n",
    "                \n",
    "                unique_mask_ids = list(set(mask_ID))\n",
    "                writer.writerow([','.join(map(str, unique_mask_ids))])\n",
    "            \n",
    "    \n",
    "                self.maskIds.append(mask_ID)\n",
    "                self.grads.append(weights)\n",
    "            \n",
    "            print(f\"Model weights and sensitivity data for client #{cid} saved to {log_file}\")\n",
    "        \n",
    "        # measurement = monitor.end_window(\"load_and_calculate_sensitivity\")\n",
    "        # print(f\"Entire load and calculate sensitivity: {measurement.time} s, {measurement.total_energy} J\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_sensitivity(self, model, dataloader):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        gradient_sums = {}\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "\n",
    "            # bias anabled\n",
    "            # if \"bias\" not in name:\n",
    "\n",
    "            gradient_sums[name] = 0.0\n",
    "\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        for inputs, labels in dataloader:\n",
    "\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "\n",
    "\n",
    "            ## For googlenet model\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "\n",
    "              outputs, aux1, aux2 = outputs\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            sensitivities = {}\n",
    "\n",
    "            for name, parameter in model.named_parameters():\n",
    "\n",
    "                # bias anabled\n",
    "                # if \"bias\" not in name and parameter.requires_grad:  # Exclude biases\n",
    "\n",
    "\n",
    "\n",
    "                ## For googlenet model\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "                if parameter.grad is not None:\n",
    "\n",
    "                  grads = parameter.grad.abs().view(-1).cpu().numpy()\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "                grads = parameter.grad.abs().view(-1).cpu().numpy()\n",
    "\n",
    "                # grads = parameter.abs().view(-1).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "                for i, grad in enumerate(grads):\n",
    "\n",
    "                    sensitivities[(name, i)] = grad\n",
    "\n",
    "\n",
    "\n",
    "            return sensitivities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_maskIds(self, sensitivity_values_node, weight_id_map):\n",
    "\n",
    "        num_weights = len(sensitivity_values_node)\n",
    "\n",
    "        top_k = int(np.ceil(self.percentage * num_weights / 100))\n",
    "\n",
    "        self.Mask_Number = top_k\n",
    "\n",
    "        sorted_weights = sorted(\n",
    "\n",
    "            sensitivity_values_node.items(), key=lambda item: item[1], reverse=True\n",
    "\n",
    "        )[:top_k]\n",
    "\n",
    "\n",
    "\n",
    "        weights = [weight for (layer, index), weight in sensitivity_values_node.items()]\n",
    "\n",
    "\n",
    "\n",
    "        top_weight_ids = [weight_id_map[layer][index] for (layer, index), _ in sorted_weights]\n",
    "\n",
    "\n",
    "\n",
    "        return top_weight_ids, weights\n",
    "\n",
    "\n",
    "\n",
    "    def normalize_distance(self, distances):\n",
    "\n",
    "        min1 = np.min(np.ma.masked_equal(distances, 0))\n",
    "\n",
    "        max1 = np.max(np.ma.masked_equal(distances, 0))\n",
    "\n",
    "        normal_distances = np.zeros((self.num_nodes, self.num_nodes))\n",
    "\n",
    "        for i in range(self.num_nodes):\n",
    "\n",
    "            normal_distances[i][i] = 0\n",
    "\n",
    "            for j in range(i + 1, self.num_nodes):\n",
    "\n",
    "                normal_distances[i][j] = normal_distances[j][i] = (\n",
    "\n",
    "                    distances[i][j] - min1\n",
    "\n",
    "                ) / (max1 - min1)\n",
    "\n",
    "\n",
    "\n",
    "        return normal_distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def normalize(self, distances, sensitive):\n",
    "\n",
    "        normal_distances = np.zeros((self.num_nodes, self.num_nodes))\n",
    "\n",
    "        for i in range(self.num_nodes):\n",
    "\n",
    "            normal_distances[i][i] = 0\n",
    "\n",
    "            for j in range(i + 1, self.num_nodes):\n",
    "\n",
    "                normal_distances[i][j] = normal_distances[j][i] = distances[i][j] / len(sensitive)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return normal_distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_common_ids(self, index1, index2):\n",
    "\n",
    "        arr1 = self.maskIds[index1]\n",
    "\n",
    "        arr2 = self.maskIds[index2]\n",
    "\n",
    "        sarr1 = set(arr1)\n",
    "\n",
    "        sarr2 = set(arr2)\n",
    "\n",
    "        inter = sarr1.intersection(sarr2)\n",
    "        \n",
    "        similarity1 = len(inter)\n",
    "\n",
    "        # print(\"similarity for----------------\",sarr1,sarr2,inter,similarity1)\n",
    "\n",
    "        return similarity1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def cosine_similarity(self, index1, index2):\n",
    "\n",
    "        dot_product = 0.0\n",
    "\n",
    "        norm1 = 0.0\n",
    "\n",
    "        norm2 = 0.0\n",
    "\n",
    "        arr1 = self.maskIds[index1]\n",
    "\n",
    "        arr2 = self.maskIds[index2]\n",
    "\n",
    "        for i in range(len(self.maskIds)):\n",
    "\n",
    "            dot_product += (arr1[i] * arr2[i]).sum().item()\n",
    "\n",
    "            norm1 += (arr1[i] ** 2).sum().item()\n",
    "\n",
    "            norm2 += (arr2[i] ** 2).sum().item()\n",
    "\n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "\n",
    "            return 0\n",
    "\n",
    "        return dot_product / (np.sqrt(norm1) * np.sqrt(norm2))\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_distance(\n",
    "\n",
    "        self,\n",
    "\n",
    "    ):\n",
    "\n",
    "        similarity_matrix = np.zeros((self.num_nodes, self.num_nodes))\n",
    "\n",
    "        for i in range(self.num_nodes):\n",
    "\n",
    "            for j in range(i + 1, self.num_nodes):\n",
    "\n",
    "                similarity = self.calculate_common_ids(i, j)\n",
    "                \n",
    "                # arr1=self.grads[i]\n",
    "\n",
    "                # arr2=self.grads[j]\n",
    "\n",
    "                # similarity = 1 - spatial.distance.cosine(arr1, arr2)\n",
    "\n",
    "                similarity_matrix[i, j] = similarity\n",
    "\n",
    "                similarity_matrix[j, i] = similarity\n",
    "\n",
    "                # print(f'similarity{i},{j} is {similarity_matrix[i, j]}')\n",
    "\n",
    "            similarity_matrix[i, i] = self.Mask_Number\n",
    "\n",
    "        # print(\"similarity:\",similarity_matrix)\n",
    "\n",
    "        max_distances = -similarity_matrix\n",
    "\n",
    "        distances = self.Mask_Number - similarity_matrix\n",
    "\n",
    "        # print(\"before normalized:\",distances)\n",
    "\n",
    "        # normal_distances = self.normalize_distance(distances)\n",
    "\n",
    "        return distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def index_to_value(self, groups):\n",
    "\n",
    "        value_groups = []\n",
    "\n",
    "        for group in groups:\n",
    "\n",
    "            list1 = []\n",
    "\n",
    "            for index in group:\n",
    "\n",
    "                list1.append(self.clients[index])\n",
    "\n",
    "            value_groups.append(list1)\n",
    "\n",
    "        return value_groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def make_clusters(self):\n",
    "\n",
    "        normal_distances = (self.distances + self.distances.T) / 2\n",
    "\n",
    "        # print(f'normal distances:{normal_distances}')\n",
    "\n",
    "        np.fill_diagonal(normal_distances, 0)\n",
    "\n",
    "        print(self.normalize(normal_distances, self.maskIds[0]))\n",
    "\n",
    "        # kmeans = KMeans(n_clusters=self.Cluster_number)\n",
    "\n",
    "        affinity_propagation = AffinityPropagation(affinity=\"precomputed\")\n",
    "\n",
    "        # clusters = kmeans.fit_predict(normal_distances)\n",
    "\n",
    "        normal_distances = -normal_distances\n",
    "\n",
    "        clusters = affinity_propagation.fit_predict(normal_distances)\n",
    "\n",
    "        print(f\"cluster results:{clusters}\")\n",
    "\n",
    "        # Find the maximum cluster label from the assigned labels\n",
    "\n",
    "        max_label = max(clusters)\n",
    "\n",
    "        # Assign unique positive labels to noise points (initially labeled as -1)\n",
    "\n",
    "        noise_indices = clusters == -1\n",
    "\n",
    "        unique_noise_labels = np.arange(\n",
    "\n",
    "            max_label + 1, max_label + 1 + np.sum(noise_indices)\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        clusters[noise_indices] = unique_noise_labels\n",
    "\n",
    "        cluster_list = [\n",
    "\n",
    "            np.where(clusters == cluster_id)[0].tolist()\n",
    "\n",
    "            for cluster_id in range(find_num_cluster(clusters))\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"clusters with location:\",cluster_list)\n",
    "\n",
    "        # print(\"clusters with clients:\", self.index_to_value(cluster_list))\n",
    "\n",
    "\n",
    "\n",
    "        cluster_list = self.index_to_value(cluster_list)\n",
    "\n",
    "\n",
    "\n",
    "        return cluster_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Federated Learning</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.546365Z",
     "iopub.status.busy": "2024-12-09T05:33:39.546049Z",
     "iopub.status.idle": "2024-12-09T05:33:39.562566Z",
     "shell.execute_reply": "2024-12-09T05:33:39.561764Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.546330Z"
    },
    "id": "DyY6UPxMw0EA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FL:\n",
    "\n",
    "    def __init__(\n",
    "\n",
    "        self,\n",
    "\n",
    "        clients,\n",
    "\n",
    "        client_initial_models,\n",
    "\n",
    "        round_number,\n",
    "\n",
    "        trainloaders,\n",
    "\n",
    "        testloaders,\n",
    "\n",
    "        Sensitivity_percentage,\n",
    "\n",
    "    ):\n",
    "\n",
    "        self.clients = clients\n",
    "\n",
    "        self.num_clients = len(clients)\n",
    "\n",
    "        self.client_initial_models = client_initial_models\n",
    "\n",
    "        self.Sensitivity_percentage = Sensitivity_percentage\n",
    "\n",
    "        self.trainloaders = trainloaders\n",
    "\n",
    "        self.testloaders = testloaders\n",
    "\n",
    "        self.round_number = round_number\n",
    "\n",
    "        self.global_model = None\n",
    "\n",
    "        self.clustering_result = None\n",
    "\n",
    "        self.client_obj_list = []\n",
    "\n",
    "        self.accuracies = {}\n",
    "\n",
    "        self.training()\n",
    "\n",
    "\n",
    "\n",
    "    def training(self):\n",
    "\n",
    "        # print(\"-----------FL class->training\")\n",
    "        # print(\"len clients\", len(self.clients))\n",
    "        # print(\"len models\", len(self.client_initial_models))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        for cid in self.clients:\n",
    "\n",
    "            print(\"cid is:\", cid)\n",
    "\n",
    "            client = Client(\n",
    "\n",
    "                self.client_initial_models[self.clients.index(int(cid))],\n",
    "\n",
    "                cid,\n",
    "\n",
    "                self.trainloaders[int(cid)],\n",
    "\n",
    "                self.testloaders[int(cid)],\n",
    "\n",
    "            )\n",
    "\n",
    "            self.client_obj_list.append(client)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        global_model = Net()\n",
    "\n",
    "\n",
    "\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "        # c = {key: torch.zeros_like(val) for key, val in global_model.state_dict().items()}\n",
    "\n",
    "        # client_controls = [{key: torch.zeros_like(val) for key, val in global_model.state_dict().items()} for _ in range(len(self.clients))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        for r in range(self.round_number):\n",
    "\n",
    "            print(f\"\\nRound {r+1}/{self.round_number}\")\n",
    "\n",
    "            server = Server()\n",
    "\n",
    "            global_accuracy = 0\n",
    "\n",
    "            index = 0\n",
    "\n",
    "            for cid in self.clients:\n",
    "\n",
    "                # self.client_obj_list[cid].net=copy.deepcopy(global_model)\n",
    "\n",
    "                # self.client_obj_list[cid]=client\n",
    "\n",
    "                # client.setting_parameters(global_model)\n",
    "\n",
    "\n",
    "                # monitor.begin_window(\"trianing_testing\")\n",
    "                train_acc, test_acc = self.client_obj_list[self.clients.index(cid)].Train_test_and_return_acc()\n",
    "                # measurement = monitor.end_window(\"trianing_testing\")\n",
    "                # print(f\"Entire trianing_testing for client #{cid}: {measurement.time} s, {measurement.total_energy} J on round {r+1}\")\n",
    "                print(\"_____________________________________________________________________________________________________________\")\n",
    "                # train_acc, test_acc = self.client_obj_list[self.clients.index(cid)].train_and_test_fedprox(global_model)\n",
    "\n",
    "                # train_acc, test_acc, c = self.client_obj_list[self.clients.index(cid)].train_and_test_scaffold(global_model, client_controls[cid], c)\n",
    "\n",
    "                print(f\"node {cid}: train_acc: {train_acc}, test_acc:{test_acc}\")\n",
    "\n",
    "                with open(log_file, \"a\") as f:\n",
    "\n",
    "                    f.write(f\"\\nNode {cid} - Round {r+1}: Train Accuracy: {train_acc}%, Test Accuracy: {test_acc}%\")\n",
    "\n",
    "                global_accuracy += test_acc\n",
    "\n",
    "                server.append_model(self.client_obj_list[self.clients.index(cid)].net)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            global_model = server.aggregate()\n",
    "\n",
    "            # global_model = server.aggregate_prox(global_model)\n",
    "\n",
    "            end_time = datetime.now()\n",
    "\n",
    "            execution_time = end_time - start_time\n",
    "\n",
    "            print(\"time\", execution_time)\n",
    "\n",
    "\n",
    "\n",
    "            with open(log_file, \"a\") as f:\n",
    "\n",
    "                f.write(f\"\\n Exe FL Round Time: {execution_time}\")\n",
    "\n",
    "            # global_model, c = server.aggregate_scaffold(global_model, client_controls, c)\n",
    "\n",
    "            print(\"global acc:\", global_accuracy / self.num_clients)\n",
    "\n",
    "            with open(log_file, \"a\") as f:\n",
    "\n",
    "                f.write(f\"\\nGlobal Model of {self.num_clients}- Round {r+1}: Test Accuracy is: {global_accuracy/self.num_clients}%\")\n",
    "\n",
    "\n",
    "\n",
    "            for cid in self.clients:\n",
    "                # !!!!!!!!!!!!!!!!!!\n",
    "                model_path = f\"models/before_agg/node_{cid}.pth\"\n",
    "                torch.save(self.client_obj_list[self.clients.index(cid)].net.state_dict(), model_path)\n",
    "                self.client_obj_list[self.clients.index(cid)].net = copy.deepcopy(global_model)\n",
    "\n",
    "\n",
    "\n",
    "        # filtered_c =  [sublist for sublist in c if sublist]\n",
    "\n",
    "\n",
    "\n",
    "        self.global_model = global_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Loading & Saving</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.564155Z",
     "iopub.status.busy": "2024-12-09T05:33:39.563846Z",
     "iopub.status.idle": "2024-12-09T05:33:39.578480Z",
     "shell.execute_reply": "2024-12-09T05:33:39.577577Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.564120Z"
    },
    "id": "LazN3rY5xDiZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_torch_model(node_id):\n",
    "\n",
    "    model_path = f\"models/node_{node_id}.pth\"\n",
    "\n",
    "    model = torch.load(model_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def save_torch_model(model, node_id):\n",
    "\n",
    "    model_path = f\"models/node_{node_id}.pth\"\n",
    "\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "\n",
    "\n",
    "def save_model_param(model, node_id, round_number):\n",
    "\n",
    "    model_path = f\"models/node_{node_id}_round_{round_number}.pth\"\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Non-IID Distribution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST, CIFAR10, SVHN, FashionMNIST, CIFAR100, ImageFolder, DatasetFolder, utils\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.datasets.utils import download_file_from_google_drive, check_integrity\n",
    "from functools import partial\n",
    "from typing import Optional, Callable\n",
    "from torch.utils.model_zoo import tqdm\n",
    "import PIL\n",
    "import tarfile\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import logging\n",
    "import torchvision.datasets.utils as utils\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "def mkdirs(dirpath):\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "class CustomTensorDataset(data.TensorDataset):\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors) + (index,)\n",
    "\n",
    "\n",
    "class MNIST_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=False):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        mnist_dataobj = MNIST(self.root, self.train, self.transform, self.target_transform, self.download)\n",
    "\n",
    "        # if self.train:\n",
    "        #     data = mnist_dataobj.train_data\n",
    "        #     target = mnist_dataobj.train_labels\n",
    "        # else:\n",
    "        #     data = mnist_dataobj.test_data\n",
    "        #     target = mnist_dataobj.test_labels\n",
    "\n",
    "        data = mnist_dataobj.data\n",
    "        target = mnist_dataobj.targets\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        # print(\"mnist img:\", img)\n",
    "        # print(\"mnist target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class FashionMNIST_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=False):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        mnist_dataobj = FashionMNIST(self.root, self.train, self.transform, self.target_transform, self.download)\n",
    "\n",
    "        # if self.train:\n",
    "        #     data = mnist_dataobj.train_data\n",
    "        #     target = mnist_dataobj.train_labels\n",
    "        # else:\n",
    "        #     data = mnist_dataobj.test_data\n",
    "        #     target = mnist_dataobj.test_labels\n",
    "\n",
    "        data = mnist_dataobj.data\n",
    "        target = mnist_dataobj.targets\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        # print(\"mnist img:\", img)\n",
    "        # print(\"mnist target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class SVHN_custom(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=False):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "        if self.train is True:\n",
    "            # svhn_dataobj1 = SVHN(self.root, 'train', self.transform, self.target_transform, self.download)\n",
    "            # svhn_dataobj2 = SVHN(self.root, 'extra', self.transform, self.target_transform, self.download)\n",
    "            # data = np.concatenate((svhn_dataobj1.data, svhn_dataobj2.data), axis=0)\n",
    "            # target = np.concatenate((svhn_dataobj1.labels, svhn_dataobj2.labels), axis=0)\n",
    "\n",
    "            svhn_dataobj = SVHN(self.root, 'train', self.transform, self.target_transform, self.download)\n",
    "            data = svhn_dataobj.data\n",
    "            target = svhn_dataobj.labels\n",
    "        else:\n",
    "            svhn_dataobj = SVHN(self.root, 'test', self.transform, self.target_transform, self.download)\n",
    "            data = svhn_dataobj.data\n",
    "            target = svhn_dataobj.labels\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "        # print(\"svhn data:\", data)\n",
    "        # print(\"len svhn data:\", len(data))\n",
    "        # print(\"type svhn data:\", type(data))\n",
    "        # print(\"svhn target:\", target)\n",
    "        # print(\"type svhn target\", type(target))\n",
    "        return data, target\n",
    "\n",
    "    # def truncate_channel(self, index):\n",
    "    #     for i in range(index.shape[0]):\n",
    "    #         gs_index = index[i]\n",
    "    #         self.data[gs_index, :, :, 1] = 0.0\n",
    "    #         self.data[gs_index, :, :, 2] = 0.0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "        # print(\"svhn img:\", img)\n",
    "        # print(\"svhn target:\", target)\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# torchvision CelebA\n",
    "class CelebA_custom(VisionDataset):\n",
    "    \"\"\"`Large-scale CelebFaces Attributes (CelebA) Dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        split (string): One of {'train', 'valid', 'test', 'all'}.\n",
    "            Accordingly dataset is selected.\n",
    "        target_type (string or list, optional): Type of target to use, ``attr``, ``identity``, ``bbox``,\n",
    "            or ``landmarks``. Can also be a list to output a tuple with all specified target types.\n",
    "            The targets represent:\n",
    "                ``attr`` (np.array shape=(40,) dtype=int): binary (0, 1) labels for attributes\n",
    "                ``identity`` (int): label for each person (data points with the same identity are the same person)\n",
    "                ``bbox`` (np.array shape=(4,) dtype=int): bounding box (x, y, width, height)\n",
    "                ``landmarks`` (np.array shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x,\n",
    "                    righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y)\n",
    "            Defaults to ``attr``. If empty, ``None`` will be returned as target.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "\n",
    "    base_folder = \"celeba\"\n",
    "    # There currently does not appear to be a easy way to extract 7z in python (without introducing additional\n",
    "    # dependencies). The \"in-the-wild\" (not aligned+cropped) images are only in 7z, so they are not available\n",
    "    # right now.\n",
    "    file_list = [\n",
    "        # File ID                         MD5 Hash                            Filename\n",
    "        (\"0B7EVK8r0v71pZjFTYXZWM3FlRnM\", \"00d2c5bc6d35e252742224ab0c1e8fcb\", \"img_align_celeba.zip\"),\n",
    "        # (\"0B7EVK8r0v71pbWNEUjJKdDQ3dGc\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_align_celeba_png.7z\"),\n",
    "        # (\"0B7EVK8r0v71peklHb0pGdDl6R28\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_celeba.7z\"),\n",
    "        (\"0B7EVK8r0v71pblRyaVFSWGxPY0U\", \"75e246fa4810816ffd6ee81facbd244c\", \"list_attr_celeba.txt\"),\n",
    "        (\"1_ee_0u7vcNLOfNLegJRHmolfH5ICW-XS\", \"32bd1bd63d3c78cd57e08160ec5ed1e2\", \"identity_CelebA.txt\"),\n",
    "        (\"0B7EVK8r0v71pbThiMVRxWXZ4dU0\", \"00566efa6fedff7a56946cd1c10f1c16\", \"list_bbox_celeba.txt\"),\n",
    "        (\"0B7EVK8r0v71pd0FJY3Blby1HUTQ\", \"cc24ecafdb5b50baae59b03474781f8c\", \"list_landmarks_align_celeba.txt\"),\n",
    "        # (\"0B7EVK8r0v71pTzJIdlJWdHczRlU\", \"063ee6ddb681f96bc9ca28c6febb9d1a\", \"list_landmarks_celeba.txt\"),\n",
    "        (\"0B7EVK8r0v71pY0NSMzRuSXJEVkk\", \"d32c9cbf5e040fd4025c592c306e6668\", \"list_eval_partition.txt\"),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, split=\"train\", target_type=\"attr\", transform=None,\n",
    "                 target_transform=None, download=False):\n",
    "        import pandas\n",
    "        super(CelebA_custom, self).__init__(root, transform=transform,\n",
    "                                     target_transform=target_transform)\n",
    "        self.split = split\n",
    "        if isinstance(target_type, list):\n",
    "            self.target_type = target_type\n",
    "        else:\n",
    "            self.target_type = [target_type]\n",
    "\n",
    "        if not self.target_type and self.target_transform is not None:\n",
    "            raise RuntimeError('target_transform is specified but target_type is empty')\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        split_map = {\n",
    "            \"train\": 0,\n",
    "            \"valid\": 1,\n",
    "            \"test\": 2,\n",
    "            \"all\": None,\n",
    "        }\n",
    "        split = split_map[split.lower()]\n",
    "\n",
    "        fn = partial(os.path.join, self.root, self.base_folder)\n",
    "        splits = pandas.read_csv(fn(\"list_eval_partition.txt\"), delim_whitespace=True, header=None, index_col=0)\n",
    "        identity = pandas.read_csv(fn(\"identity_CelebA.txt\"), delim_whitespace=True, header=None, index_col=0)\n",
    "        bbox = pandas.read_csv(fn(\"list_bbox_celeba.txt\"), delim_whitespace=True, header=1, index_col=0)\n",
    "        landmarks_align = pandas.read_csv(fn(\"list_landmarks_align_celeba.txt\"), delim_whitespace=True, header=1)\n",
    "        attr = pandas.read_csv(fn(\"list_attr_celeba.txt\"), delim_whitespace=True, header=1)\n",
    "\n",
    "        mask = slice(None) if split is None else (splits[1] == split)\n",
    "\n",
    "        self.filename = splits[mask].index.values\n",
    "        self.identity = torch.as_tensor(identity[mask].values)\n",
    "        self.bbox = torch.as_tensor(bbox[mask].values)\n",
    "        self.landmarks_align = torch.as_tensor(landmarks_align[mask].values)\n",
    "        self.attr = torch.as_tensor(attr[mask].values)\n",
    "        self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n",
    "        self.attr_names = list(attr.columns)\n",
    "        self.gender_index = self.attr_names.index('Male')\n",
    "        self.dataidxs = dataidxs\n",
    "        if self.dataidxs is None:\n",
    "            self.target = self.attr[:, self.gender_index:self.gender_index + 1].reshape(-1)\n",
    "        else:\n",
    "            self.target = self.attr[self.dataidxs, self.gender_index:self.gender_index + 1].reshape(-1)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        for (_, md5, filename) in self.file_list:\n",
    "            fpath = os.path.join(self.root, self.base_folder, filename)\n",
    "            _, ext = os.path.splitext(filename)\n",
    "            # Allow original archive to be deleted (zip and 7z)\n",
    "            # Only need the extracted images\n",
    "            if ext not in [\".zip\", \".7z\"] and not check_integrity(fpath, md5):\n",
    "                return False\n",
    "\n",
    "        # Should check a hash of the images\n",
    "        return os.path.isdir(os.path.join(self.root, self.base_folder, \"img_align_celeba\"))\n",
    "\n",
    "    def download(self):\n",
    "        import zipfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        for (file_id, md5, filename) in self.file_list:\n",
    "            download_file_from_google_drive(file_id, os.path.join(self.root, self.base_folder), filename, md5)\n",
    "\n",
    "        with zipfile.ZipFile(os.path.join(self.root, self.base_folder, \"img_align_celeba.zip\"), \"r\") as f:\n",
    "            f.extractall(os.path.join(self.root, self.base_folder))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.dataidxs is None:\n",
    "            X = PIL.Image.open(os.path.join(self.root, self.base_folder, \"img_align_celeba\", self.filename[index]))\n",
    "\n",
    "            target = []\n",
    "            for t in self.target_type:\n",
    "                if t == \"attr\":\n",
    "                    target.append(self.attr[index, self.gender_index])\n",
    "                elif t == \"identity\":\n",
    "                    target.append(self.identity[index, 0])\n",
    "                elif t == \"bbox\":\n",
    "                    target.append(self.bbox[index, :])\n",
    "                elif t == \"landmarks\":\n",
    "                    target.append(self.landmarks_align[index, :])\n",
    "                else:\n",
    "                    # TODO: refactor with utils.verify_str_arg\n",
    "                    raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(t))\n",
    "        else:\n",
    "            X = PIL.Image.open(os.path.join(self.root, self.base_folder, \"img_align_celeba\", self.filename[self.dataidxs[index]]))\n",
    "\n",
    "            target = []\n",
    "            for t in self.target_type:\n",
    "                if t == \"attr\":\n",
    "                    target.append(self.attr[self.dataidxs[index], self.gender_index])\n",
    "                elif t == \"identity\":\n",
    "                    target.append(self.identity[self.dataidxs[index], 0])\n",
    "                elif t == \"bbox\":\n",
    "                    target.append(self.bbox[self.dataidxs[index], :])\n",
    "                elif t == \"landmarks\":\n",
    "                    target.append(self.landmarks_align[self.dataidxs[index], :])\n",
    "                else:\n",
    "                    # TODO: refactor with utils.verify_str_arg\n",
    "                    raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(t))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        #print(\"target[0]:\", target[0])\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "        #print(\"celeba target:\", target)\n",
    "        return X, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataidxs is None:\n",
    "            return len(self.attr)\n",
    "        else:\n",
    "            return len(self.dataidxs)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return '\\n'.join(lines).format(**self.__dict__)\n",
    "\n",
    "\n",
    "\n",
    "class CIFAR10_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=False):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        cifar_dataobj = CIFAR10(self.root, self.train, self.transform, self.target_transform, self.download)\n",
    "\n",
    "        data = cifar_dataobj.data\n",
    "        target = np.array(cifar_dataobj.targets)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def truncate_channel(self, index):\n",
    "        for i in range(index.shape[0]):\n",
    "            gs_index = index[i]\n",
    "            self.data[gs_index, :, :, 1] = 0.0\n",
    "            self.data[gs_index, :, :, 2] = 0.0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        # print(\"cifar10 img:\", img)\n",
    "        # print(\"cifar10 target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def gen_bar_updater() -> Callable[[int, int, int], None]:\n",
    "    pbar = tqdm(total=None)\n",
    "\n",
    "    def bar_update(count, block_size, total_size):\n",
    "        if pbar.total is None and total_size:\n",
    "            pbar.total = total_size\n",
    "        progress_bytes = count * block_size\n",
    "        pbar.update(progress_bytes - pbar.n)\n",
    "\n",
    "    return bar_update\n",
    "\n",
    "\n",
    "def download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None) -> None:\n",
    "    \"\"\"Download a file from a url and place it in root.\n",
    "    Args:\n",
    "        url (str): URL to download file from\n",
    "        root (str): Directory to place downloaded file in\n",
    "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
    "        md5 (str, optional): MD5 checksum of the download. If None, do not check\n",
    "    \"\"\"\n",
    "    import urllib\n",
    "\n",
    "    root = os.path.expanduser(root)\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "    fpath = os.path.join(root, filename)\n",
    "\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "\n",
    "    # check if file is already present locally\n",
    "    if check_integrity(fpath, md5):\n",
    "        print('Using downloaded and verified file: ' + fpath)\n",
    "    else:   # download the file\n",
    "        try:\n",
    "            print('Downloading ' + url + ' to ' + fpath)\n",
    "            urllib.request.urlretrieve(\n",
    "                url, fpath,\n",
    "                reporthook=gen_bar_updater()\n",
    "            )\n",
    "        except (urllib.error.URLError, IOError) as e:  # type: ignore[attr-defined]\n",
    "            if url[:5] == 'https':\n",
    "                url = url.replace('https:', 'http:')\n",
    "                print('Failed download. Trying https -> http instead.'\n",
    "                      ' Downloading ' + url + ' to ' + fpath)\n",
    "                urllib.request.urlretrieve(\n",
    "                    url, fpath,\n",
    "                    reporthook=gen_bar_updater()\n",
    "                )\n",
    "            else:\n",
    "                raise e\n",
    "        # check integrity of downloaded file\n",
    "        if not check_integrity(fpath, md5):\n",
    "            raise RuntimeError(\"File not found or corrupted.\")\n",
    "\n",
    "def _is_tarxz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar.xz\")\n",
    "\n",
    "\n",
    "def _is_tar(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar\")\n",
    "\n",
    "\n",
    "def _is_targz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar.gz\")\n",
    "\n",
    "\n",
    "def _is_tgz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tgz\")\n",
    "\n",
    "\n",
    "def _is_gzip(filename: str) -> bool:\n",
    "    return filename.endswith(\".gz\") and not filename.endswith(\".tar.gz\")\n",
    "\n",
    "\n",
    "def _is_zip(filename: str) -> bool:\n",
    "    return filename.endswith(\".zip\")\n",
    "\n",
    "\n",
    "def extract_archive(from_path: str, to_path: Optional[str] = None, remove_finished: bool = False) -> None:\n",
    "    if to_path is None:\n",
    "        to_path = os.path.dirname(from_path)\n",
    "\n",
    "    if _is_tar(from_path):\n",
    "        with tarfile.open(from_path, 'r') as tar:\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_targz(from_path) or _is_tgz(from_path):\n",
    "        with tarfile.open(from_path, 'r:gz') as tar:\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_tarxz(from_path):\n",
    "        with tarfile.open(from_path, 'r:xz') as tar:\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_gzip(from_path):\n",
    "        to_path = os.path.join(to_path, os.path.splitext(os.path.basename(from_path))[0])\n",
    "        with open(to_path, \"wb\") as out_f, gzip.GzipFile(from_path) as zip_f:\n",
    "            out_f.write(zip_f.read())\n",
    "    elif _is_zip(from_path):\n",
    "        with zipfile.ZipFile(from_path, 'r') as z:\n",
    "            z.extractall(to_path)\n",
    "    else:\n",
    "        raise ValueError(\"Extraction of {} not supported\".format(from_path))\n",
    "\n",
    "    if remove_finished:\n",
    "        os.remove(from_path)\n",
    "\n",
    "\n",
    "def download_and_extract_archive(\n",
    "    url: str,\n",
    "    download_root: str,\n",
    "    extract_root: Optional[str] = None,\n",
    "    filename: Optional[str] = None,\n",
    "    md5: Optional[str] = None,\n",
    "    remove_finished: bool = False,\n",
    ") -> None:\n",
    "    download_root = os.path.expanduser(download_root)\n",
    "    if extract_root is None:\n",
    "        extract_root = download_root\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "\n",
    "    download_url(url, download_root, filename, md5)\n",
    "\n",
    "    archive = os.path.join(download_root, filename)\n",
    "    print(\"Extracting {} to {}\".format(archive, extract_root))\n",
    "    extract_archive(archive, extract_root, remove_finished)\n",
    "\n",
    "class FEMNIST(MNIST):\n",
    "    \"\"\"\n",
    "    This dataset is derived from the Leaf repository\n",
    "    (https://github.com/TalwalkarLab/leaf) pre-processing of the Extended MNIST\n",
    "    dataset, grouping examples by writer. Details about Leaf were published in\n",
    "    \"LEAF: A Benchmark for Federated Settings\" https://arxiv.org/abs/1812.01097.\n",
    "    \"\"\"\n",
    "    resources = [\n",
    "        ('https://raw.githubusercontent.com/tao-shen/FEMNIST_pytorch/master/femnist.tar.gz',\n",
    "         '59c65cec646fc57fe92d27d83afdf0ed')]\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(MNIST, self).__init__(root, transform=transform,\n",
    "                                    target_transform=target_transform)\n",
    "        self.train = train\n",
    "        self.dataidxs = dataidxs\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        if self.train:\n",
    "            data_file = self.training_file\n",
    "        else:\n",
    "            data_file = self.test_file\n",
    "\n",
    "        self.data, self.targets, self.users_index = torch.load(os.path.join(self.processed_folder, data_file))\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            self.data = self.data[self.dataidxs]\n",
    "            self.targets = self.targets[self.dataidxs]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        img = Image.fromarray(img.numpy(), mode='F')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the FEMNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        import shutil\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        mkdirs(self.raw_folder)\n",
    "        mkdirs(self.processed_folder)\n",
    "\n",
    "        # download files\n",
    "        for url, md5 in self.resources:\n",
    "            filename = url.rpartition('/')[2]\n",
    "            download_and_extract_archive(url, download_root=self.raw_folder, filename=filename, md5=md5)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('Processing...')\n",
    "        shutil.move(os.path.join(self.raw_folder, self.training_file), self.processed_folder)\n",
    "        shutil.move(os.path.join(self.raw_folder, self.test_file), self.processed_folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_exists(self) -> bool:\n",
    "        return all(\n",
    "            check_integrity(os.path.join(self.raw_folder, os.path.splitext(os.path.basename(url))[0]+os.path.splitext(os.path.basename(url))[1]))\n",
    "            for url, _ in self.resources\n",
    "        )\n",
    "\n",
    "\n",
    "class Generated(MNIST):\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(MNIST, self).__init__(root, transform=transform,\n",
    "                                    target_transform=target_transform)\n",
    "        self.train = train\n",
    "        self.dataidxs = dataidxs\n",
    "\n",
    "        if self.train:\n",
    "            self.data = np.load(\"data/generated/X_train.npy\")\n",
    "            self.targets = np.load(\"data/generated/y_train.npy\")\n",
    "        else:\n",
    "            self.data = np.load(\"data/generated/X_test.npy\")\n",
    "            self.targets = np.load(\"data/generated/y_test.npy\")\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            self.data = self.data[self.dataidxs]\n",
    "            self.targets = self.targets[self.dataidxs]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.data[index], self.targets[index]\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "\n",
    "class genData(MNIST):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    def __getitem__(self,index):\n",
    "        data, target = self.data[index], self.targets[index]\n",
    "        return data, target\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class CIFAR100_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=False):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        cifar_dataobj = CIFAR100(self.root, self.train, self.transform, self.target_transform, self.download)\n",
    "\n",
    "        if torchvision.__version__ == '0.2.1':\n",
    "            if self.train:\n",
    "                data, target = cifar_dataobj.train_data, np.array(cifar_dataobj.train_labels)\n",
    "            else:\n",
    "                data, target = cifar_dataobj.test_data, np.array(cifar_dataobj.test_labels)\n",
    "        else:\n",
    "            data = cifar_dataobj.data\n",
    "            target = np.array(cifar_dataobj.targets)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "        img = Image.fromarray(img)\n",
    "        # print(\"cifar10 img:\", img)\n",
    "        # print(\"cifar10 target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ImageFolder_custom(DatasetFolder):\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=None):\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "\n",
    "        imagefolder_obj = ImageFolder(self.root, self.transform, self.target_transform)\n",
    "        self.loader = imagefolder_obj.loader\n",
    "        if self.dataidxs is not None:\n",
    "            self.samples = np.array(imagefolder_obj.samples)[self.dataidxs]\n",
    "        else:\n",
    "            self.samples = np.array(imagefolder_obj.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.samples[index][0]\n",
    "        target = self.samples[index][1]\n",
    "        target = int(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataidxs is None:\n",
    "            return len(self.samples)\n",
    "        else:\n",
    "            return len(self.dataidxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import time\n",
    "import random\n",
    "from tinyimagenet import TinyImageNet\n",
    "\n",
    "import sklearn.datasets as sk\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def mkdirs(dirpath):\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "def load_mnist_data(datadir):\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    mnist_train_ds = MNIST_truncated(datadir, train=True, download=True, transform=transform)\n",
    "    mnist_test_ds = MNIST_truncated(datadir, train=False, download=True, transform=transform)\n",
    "\n",
    "    X_train, y_train = mnist_train_ds.data, mnist_train_ds.target\n",
    "    X_test, y_test = mnist_test_ds.data, mnist_test_ds.target\n",
    "\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "def load_fmnist_data(datadir):\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    mnist_train_ds = FashionMNIST_truncated(datadir, train=True, download=True, transform=transform)\n",
    "    mnist_test_ds = FashionMNIST_truncated(datadir, train=False, download=True, transform=transform)\n",
    "\n",
    "    X_train, y_train = mnist_train_ds.data, mnist_train_ds.target\n",
    "    X_test, y_test = mnist_test_ds.data, mnist_test_ds.target\n",
    "\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "def load_svhn_data(datadir):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()])\n",
    "    \n",
    "    \n",
    "\n",
    "    svhn_train_ds = SVHN_custom(datadir, train=True, download=True, transform=transform)\n",
    "    svhn_test_ds = SVHN_custom(datadir, train=False, download=True, transform=transform)\n",
    "\n",
    "    X_train, y_train = svhn_train_ds.data, svhn_train_ds.target\n",
    "    X_test, y_test = svhn_test_ds.data, svhn_test_ds.target\n",
    "\n",
    "    # X_train = X_train.data.numpy()\n",
    "    # y_train = y_train.data.numpy()\n",
    "    # X_test = X_test.data.numpy()\n",
    "    # y_test = y_test.data.numpy()\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_cifar10_data(datadir):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    cifar10_train_ds = CIFAR10_truncated(datadir, train=True, download=True, transform=transform)\n",
    "    cifar10_test_ds = CIFAR10_truncated(datadir, train=False, download=True, transform=transform)\n",
    "\n",
    "    X_train, y_train = cifar10_train_ds.data, cifar10_train_ds.target\n",
    "    X_test, y_test = cifar10_test_ds.data, cifar10_test_ds.target\n",
    "\n",
    "    # y_train = y_train.numpy()\n",
    "    # y_test = y_test.numpy()\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "def load_celeba_data(datadir):\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    celeba_train_ds = CelebA_custom(datadir, split='train', target_type=\"attr\", download=True, transform=transform)\n",
    "    celeba_test_ds = CelebA_custom(datadir, split='test', target_type=\"attr\", download=True, transform=transform)\n",
    "\n",
    "    gender_index = celeba_train_ds.attr_names.index('Male')\n",
    "    y_train =  celeba_train_ds.attr[:,gender_index:gender_index+1].reshape(-1)\n",
    "    y_test = celeba_test_ds.attr[:,gender_index:gender_index+1].reshape(-1)\n",
    "\n",
    "    # y_train = y_train.numpy()\n",
    "    # y_test = y_test.numpy()\n",
    "\n",
    "    return (None, y_train, None, y_test)\n",
    "\n",
    "def load_femnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    mnist_train_ds = FEMNIST(datadir, train=True, transform=transform, download=True)\n",
    "    mnist_test_ds = FEMNIST(datadir, train=False, transform=transform, download=True)\n",
    "\n",
    "    X_train, y_train, u_train = mnist_train_ds.data, mnist_train_ds.targets, mnist_train_ds.users_index\n",
    "    X_test, y_test, u_test = mnist_test_ds.data, mnist_test_ds.targets, mnist_test_ds.users_index\n",
    "\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    u_train = np.array(u_train)\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    u_test = np.array(u_test)\n",
    "\n",
    "    return (X_train, y_train, u_train, X_test, y_test, u_test)\n",
    "\n",
    "def load_cifar100_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    cifar100_train_ds = CIFAR100_truncated(datadir, train=True, download=True, transform=transform)\n",
    "    cifar100_test_ds = CIFAR100_truncated(datadir, train=False, download=True, transform=transform)\n",
    "\n",
    "    X_train, y_train = cifar100_train_ds.data, cifar100_train_ds.target\n",
    "    X_test, y_test = cifar100_test_ds.data, cifar100_test_ds.target\n",
    "\n",
    "    # y_train = y_train.numpy()\n",
    "    # y_test = y_test.numpy()\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_tinyimagenet_data(datadir):\n",
    "    split =\"val\"\n",
    "    TinyImageNet(datadir,split=split)\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    xray_train_ds = ImageFolder_custom(datadir + 'tiny-imagenet-200/train/', transform=transform)\n",
    "    xray_test_ds = ImageFolder_custom(datadir + 'tiny-imagenet-200/val/', transform=transform)\n",
    "\n",
    "    X_train, y_train = np.array([s[0] for s in xray_train_ds.samples]), np.array([int(s[1]) for s in xray_train_ds.samples])\n",
    "    X_test, y_test = np.array([s[0] for s in xray_test_ds.samples]), np.array([int(s[1]) for s in xray_test_ds.samples])\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "def record_net_data_stats(y_train, net_dataidx_map, logdir):\n",
    "\n",
    "    net_cls_counts = {}\n",
    "\n",
    "    for net_i, dataidx in net_dataidx_map.items():\n",
    "        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True)\n",
    "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
    "        net_cls_counts[net_i] = tmp\n",
    "\n",
    "    logger.info('Data statistics: %s' % str(net_cls_counts))\n",
    "\n",
    "    return net_cls_counts\n",
    "\n",
    "def partition_data(dataset, datadir, logdir, partition, n_parties, beta=0.4):\n",
    "    # Optional: set random seeds for reproducibility\n",
    "    # np.random.seed(2020)\n",
    "    # torch.manual_seed(2020)\n",
    "\n",
    "    # Initialize test data index map\n",
    "    test_dataidx_map = {}\n",
    "\n",
    "    # Load dataset\n",
    "    if dataset == 'mnist':\n",
    "        X_train, y_train, X_test, y_test = load_mnist_data(datadir)\n",
    "    elif dataset == 'fmnist':\n",
    "        X_train, y_train, X_test, y_test = load_fmnist_data(datadir)\n",
    "    elif dataset == 'cifar10':\n",
    "        X_train, y_train, X_test, y_test = load_cifar10_data(datadir)\n",
    "    elif dataset == 'svhn':\n",
    "        X_train, y_train, X_test, y_test = load_svhn_data(datadir)\n",
    "    elif dataset == 'celeba':\n",
    "        X_train, y_train, X_test, y_test = load_celeba_data(datadir)\n",
    "    elif dataset == 'femnist':\n",
    "        X_train, y_train, u_train, X_test, y_test, u_test = load_femnist_data(datadir)\n",
    "    elif dataset == 'cifar100':\n",
    "        X_train, y_train, X_test, y_test = load_cifar100_data(datadir)\n",
    "    elif dataset == 'tinyimagenet':\n",
    "        X_train, y_train, X_test, y_test = load_tinyimagenet_data(datadir)\n",
    "    elif dataset == 'generated':\n",
    "        # Code for generated dataset (omitted for brevity)\n",
    "        pass\n",
    "    # Add other datasets if needed\n",
    "\n",
    "    n_train = y_train.shape[0]\n",
    "\n",
    "    # Partition the data\n",
    "    if partition == \"homo\":\n",
    "        # Homogeneous data partition\n",
    "        idxs = np.random.permutation(n_train)\n",
    "        batch_idxs = np.array_split(idxs, n_parties)\n",
    "        net_dataidx_map = {i: batch_idxs[i] for i in range(n_parties)}\n",
    "\n",
    "    elif partition == \"noniid-labeldir\":\n",
    "        # Non-IID partition using Dirichlet distribution\n",
    "        # Code omitted for brevity\n",
    "        pass\n",
    "\n",
    "    elif partition.startswith(\"noniid-#label\") and partition[13:].isdigit():\n",
    "        # Non-IID partition where each client has a fixed number of labels\n",
    "        num = int(partition[13:])\n",
    "        if dataset in ('celeba', 'covtype', 'a9a', 'rcv1', 'SUSY'):\n",
    "            num = 1\n",
    "            K = 2\n",
    "        else:\n",
    "            if dataset == 'cifar100':\n",
    "                K = 100\n",
    "            elif dataset == 'tinyimagenet':\n",
    "                K = 200\n",
    "            else:\n",
    "                K = 10\n",
    "\n",
    "        if num == K:\n",
    "            # IID partition\n",
    "            net_dataidx_map = {i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)}\n",
    "            for i in range(K):\n",
    "                idx_k = np.where(y_train == i)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                split = np.array_split(idx_k, n_parties)\n",
    "                for j in range(n_parties):\n",
    "                    net_dataidx_map[j] = np.append(net_dataidx_map[j], split[j])\n",
    "        else:\n",
    "            times = [0 for _ in range(K)]\n",
    "            contain = []\n",
    "            for i in range(n_parties):\n",
    "                current = [i % K]\n",
    "                times[i % K] += 1\n",
    "                j = 1\n",
    "                while j < num:\n",
    "                    ind = random.randint(0, K - 1)\n",
    "                    if ind not in current:\n",
    "                        j += 1\n",
    "                        current.append(ind)\n",
    "                        times[ind] += 1\n",
    "                contain.append(current)\n",
    "\n",
    "            net_dataidx_map = {i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)}\n",
    "            test_dataidx_map = {i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)}\n",
    "\n",
    "            for i in range(K):\n",
    "                if times[i] > 0:\n",
    "                    idx_k = np.where(y_train == i)[0]\n",
    "                    idx_t = np.where(y_test == i)[0]\n",
    "                    np.random.shuffle(idx_k)\n",
    "                    np.random.shuffle(idx_t)\n",
    "                    split = np.array_split(idx_k, times[i])\n",
    "                    splitt = np.array_split(idx_t, times[i])\n",
    "                    ids = 0\n",
    "                    for j in range(n_parties):\n",
    "                        if i in contain[j]:\n",
    "                            net_dataidx_map[j] = np.append(net_dataidx_map[j], split[ids])\n",
    "                            test_dataidx_map[j] = np.append(test_dataidx_map[j], splitt[ids])\n",
    "                            ids += 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown partition method: {partition}\")\n",
    "\n",
    "    traindata_cls_counts = record_net_data_stats(y_train, net_dataidx_map, logdir)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, net_dataidx_map, test_dataidx_map, traindata_cls_counts\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1., net_id=None, total=0):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.net_id = net_id\n",
    "        self.num = int(sqrt(total))\n",
    "        if self.num * self.num < total:\n",
    "            self.num = self.num + 1\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        if self.net_id is None:\n",
    "            return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "        else:\n",
    "            tmp = torch.randn(tensor.size())\n",
    "            filt = torch.zeros(tensor.size())\n",
    "            size = int(28 / self.num)\n",
    "            row = int(self.net_id / size)\n",
    "            col = self.net_id % size\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    filt[:,row*size+i,col*size+j] = 1\n",
    "            tmp = tmp * filt\n",
    "            return tensor + tmp * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "def get_dataloader(dataset, datadir, train_bs, test_bs, dataidxs=None, testidxs=None, noise_level=0, net_id=None, total=0):\n",
    "    if dataset in ('mnist', 'femnist', 'fmnist', 'cifar10', 'svhn', 'generated', 'covtype', 'a9a', 'rcv1', 'SUSY', 'cifar100', 'tinyimagenet'):\n",
    "        if dataset == 'mnist':\n",
    "            dl_obj = MNIST_truncated\n",
    "\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)])\n",
    "\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)])\n",
    "\n",
    "        elif dataset == 'femnist':\n",
    "            dl_obj = FEMNIST\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)])\n",
    "\n",
    "        elif dataset == 'fmnist':\n",
    "            dl_obj = FashionMNIST_truncated\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)])\n",
    "\n",
    "        elif dataset == 'svhn':\n",
    "            dl_obj = SVHN_custom\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    \n",
    "                transforms.ToTensor(),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)])\n",
    "\n",
    "\n",
    "        elif dataset == 'cifar10':\n",
    "            print(\"in cifar10\")\n",
    "            dl_obj = CIFAR10_truncated\n",
    "            \n",
    "            transform_train = transforms.Compose([\n",
    "                #transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: F.pad(\n",
    "                    Variable(x.unsqueeze(0), requires_grad=False),\n",
    "                    (4, 4, 4, 4), mode='reflect').data.squeeze()),\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomCrop(32),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)\n",
    "            ])\n",
    "            # data prep for test set\n",
    "            transform_test = transforms.Compose([\n",
    "                #transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                AddGaussianNoise(0., noise_level, net_id, total)\n",
    "                ])\n",
    "            \n",
    "                                    \n",
    "\n",
    "        elif dataset == 'cifar100':\n",
    "            print(\"in 100\")\n",
    "            dl_obj = CIFAR100_truncated\n",
    "\n",
    "            normalize = transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                             std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404])\n",
    "            # transform_train = transforms.Compose([\n",
    "            #     transforms.RandomCrop(32),\n",
    "            #     transforms.RandomHorizontalFlip(),\n",
    "            #     transforms.ToTensor(),\n",
    "            #     normalize\n",
    "            # ])\n",
    "            transform_train = transforms.Compose([\n",
    "                # transforms.ToPILImage(),\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "            # data prep for test set\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize])\n",
    "        elif dataset == 'tinyimagenet':\n",
    "            dl_obj = ImageFolder_custom\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.Resize(64),\n",
    "                transforms.RandomCrop(64, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4802, 0.4481, 0.3975), (0.2770, 0.2691, 0.2821)),\n",
    "            ])\n",
    "\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.Resize(64),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4802, 0.4481, 0.3975), (0.2770, 0.2691, 0.2821)),\n",
    "            ])\n",
    "\n",
    "        else:\n",
    "            dl_obj = Generated\n",
    "            transform_train = None\n",
    "            transform_test = None\n",
    "\n",
    "\n",
    "        if dataset == \"tinyimagenet\":\n",
    "            train_ds = dl_obj(datadir+'tiny-imagenet-200/train/', dataidxs=dataidxs, transform=transform_train)\n",
    "            test_ds = dl_obj(datadir+'tiny-imagenet-200/val/', transform=transform_test)\n",
    "        else:\n",
    "            print(\"dir\", datadir)\n",
    "            train_ds = dl_obj(datadir, dataidxs=dataidxs, train=True, transform=transform_train, download=True)\n",
    "            test_ds = dl_obj(datadir, dataidxs=testidxs, train=False, transform=transform_test, download=True)\n",
    "\n",
    "        train_dl = data.DataLoader(dataset=train_ds, batch_size=train_bs, shuffle=True, drop_last=False)\n",
    "        test_dl = data.DataLoader(dataset=test_ds, batch_size=test_bs, shuffle=False, drop_last=False)\n",
    "        print(train_ds, \"train ds\")\n",
    "\n",
    "    return train_dl, test_dl, train_ds, test_ds\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(num_clients):\n",
    "\n",
    "  X_train, y_train, X_test, y_test, net_dataidx_map, test_dataidx_map, traindata_cls_counts = partition_data(dataset='tinyimagenet', datadir='./data/', logdir='./logs/', partition='noniid-#label2', n_parties=10)\n",
    "  print(\"shapes\", X_train.shape, y_train.shape)\n",
    "  trainloaders = []\n",
    "  testloaders = []\n",
    "  for client_id in range(num_clients):\n",
    "\n",
    "    dataidxs = net_dataidx_map[client_id]\n",
    "    testidxs = test_dataidx_map[client_id]\n",
    "\n",
    "    train_dl_local, test_dl_local, train_ds_local, test_ds_local = get_dataloader(dataset='tinyimagenet', datadir='./data/', train_bs=128, test_bs=128, dataidxs=dataidxs, testidxs=testidxs)\n",
    "    trainloaders.append(train_dl_local)\n",
    "    testloaders.append(test_dl_local)\n",
    "\n",
    "  return trainloaders, testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.579632Z",
     "iopub.status.busy": "2024-12-09T05:33:39.579393Z",
     "iopub.status.idle": "2024-12-09T05:33:39.590076Z",
     "shell.execute_reply": "2024-12-09T05:33:39.589219Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.579608Z"
    },
    "id": "-IvzdpYcxGZx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    trainloaders, testloaders = get_loaders(10)\n",
    "    return trainloaders, testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:39.591397Z",
     "iopub.status.busy": "2024-12-09T05:33:39.591067Z",
     "iopub.status.idle": "2024-12-09T05:33:47.647865Z",
     "shell.execute_reply": "2024-12-09T05:33:47.646680Z",
     "shell.execute_reply.started": "2024-12-09T05:33:39.591361Z"
    },
    "id": "ORJsNkg1xMY4",
    "outputId": "99812738-52e5-4abd-bafb-56edba8a171d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainloaders,testloaders=load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Visualization</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:47:55.640148Z",
     "iopub.status.busy": "2024-12-09T05:47:55.639276Z",
     "iopub.status.idle": "2024-12-09T05:47:55.660170Z",
     "shell.execute_reply": "2024-12-09T05:47:55.659322Z",
     "shell.execute_reply.started": "2024-12-09T05:47:55.640096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    def __init__(self, trainloaders):\n",
    "        self.trainloaders = trainloaders\n",
    "    def count_classes(self):\n",
    "        # Initialize a list to store class counts for each DataLoader\n",
    "        class_counts = []\n",
    "        for loader in self.trainloaders:\n",
    "            # Initialize counts for this DataLoader\n",
    "            counts = np.zeros(10, dtype=int)\n",
    "            # Iterate through all batches in the DataLoader\n",
    "            for _, labels in loader:\n",
    "                # Count occurrences of each class in this batch and add to counts\n",
    "                for label in labels:\n",
    "                    counts[label] += 1\n",
    "            class_counts.append(counts)\n",
    "        return class_counts\n",
    "\n",
    "    def plot_class_distribution(self, dataset_type='Train', save_tikz=False, tikz_filename='class_distribution.tex'):\n",
    "        class_counts = self.count_classes()\n",
    "        num_classes = NUMBER_OF_CLASSES\n",
    "        labels = [\n",
    "            \"airplane\",\n",
    "            \"automobile\",\n",
    "            \"bird\",\n",
    "            \"cat\",\n",
    "            \"deer\",\n",
    "            \"dog\",\n",
    "            \"frog\",\n",
    "            \"horse\",\n",
    "            \"ship\",\n",
    "            \"truck\",\n",
    "        ]\n",
    "        num_nodes = len(class_counts)\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        width = 0.6  # Width of the bars\n",
    "        # Convert class_counts to a NumPy array for easier manipulation\n",
    "        counts = np.array(class_counts)  # Shape: (num_nodes, num_classes)\n",
    "        x = np.arange(num_nodes)  # Positions along x-axis representing nodes\n",
    "        # Colors for each class\n",
    "        colors = plt.cm.tab10.colors  # Use a colormap with enough distinct colors\n",
    "        # Plot data\n",
    "        bottom = np.zeros(num_nodes)  # Initialize bottom for stacking\n",
    "        for i in range(num_classes):\n",
    "            counts_per_class = counts[:, i]  # Counts for class i across nodes\n",
    "            ax.bar(\n",
    "                x,\n",
    "                counts_per_class,\n",
    "                width,\n",
    "                bottom=bottom,\n",
    "                label=labels[i],\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor='white'\n",
    "            )\n",
    "            bottom += counts_per_class  # Update bottom for next class\n",
    "        ax.set_xlabel(\"Nodes\")\n",
    "        ax.set_ylabel(\"Number of Samples\")\n",
    "        ax.set_title(f\"Distribution of {dataset_type} Classes Across Different Nodes\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([f\"{i+1}\" for i in range(num_nodes)], rotation=0)\n",
    "        # Place the legend for classes outside the plot area at the top right\n",
    "        ax.legend(\n",
    "            title=\"Classes\",\n",
    "            bbox_to_anchor=(1.05, 1),\n",
    "            loc='upper left',\n",
    "            borderaxespad=0.,\n",
    "            frameon=False\n",
    "        )\n",
    "        # Adjust the subplot to make room for the legend on the right\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(right=0.75)  # Adjust right boundary to make space for legend\n",
    "        # Save the figure as TikZ code if requested\n",
    "        if save_tikz:\n",
    "            # Adjust parameters for the TikZ export if necessary\n",
    "            tikzplotlib.save(\n",
    "                tikz_filename,\n",
    "                figure=fig,\n",
    "                encoding='utf-8',\n",
    "                axis_width='\\\\figurewidth',   # Use LaTeX width variables if desired\n",
    "                axis_height='\\\\figureheight',\n",
    "                textsize=10.0                  # Adjust text size if needed\n",
    "            )\n",
    "            print(f\"TikZ code saved to {tikz_filename}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:48:37.189405Z",
     "iopub.status.busy": "2024-12-09T05:48:37.189041Z",
     "iopub.status.idle": "2024-12-09T05:48:40.830205Z",
     "shell.execute_reply": "2024-12-09T05:48:40.828992Z",
     "shell.execute_reply.started": "2024-12-09T05:48:37.189372Z"
    },
    "id": "KS4EpcfYxOK6",
    "outputId": "768f2820-33ea-44f5-8737-6f763f216c92",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizer(trainloaders).plot_class_distribution()\n",
    "\n",
    "# Visualizer(testloaders).plot_class_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Executing</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:33:51.247202Z",
     "iopub.status.busy": "2024-12-09T05:33:51.246884Z",
     "iopub.status.idle": "2024-12-09T05:35:38.001063Z",
     "shell.execute_reply": "2024-12-09T05:35:37.999815Z",
     "shell.execute_reply.started": "2024-12-09T05:33:51.247162Z"
    },
    "id": "KTKWmDCUxXfO",
    "outputId": "147c0c4f-07b1-4428-a9a2-d43039f7249b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clusters=[]\n",
    "initial = [i for i in range(num_clients)]\n",
    "clusters.append(initial)\n",
    "\n",
    "\n",
    "def generate_initial_models(step,cluster,client_IDs,client_Models):\n",
    "    print(\"-------------in initial genertaio\")\n",
    "    print(\"cluster\", cluster)\n",
    "    print(\"clientIDs\", client_IDs)\n",
    "    print(\"len_client_models(should be 10):\",len(client_Models))\n",
    "    list1=[]\n",
    "\n",
    "    if step==0:\n",
    "        for member in range(len(cluster)):\n",
    "            list1.append(Net())\n",
    "    else:\n",
    "        for index in cluster:\n",
    "            list1.append(client_Models[client_IDs.index(index)])\n",
    "    return list1\n",
    "\n",
    "\n",
    "client_Models=[]\n",
    "client_copy_models = []\n",
    "\n",
    "for step in range(Clustering_period):\n",
    "    client_copy_models=copy.deepcopy(client_Models)\n",
    "    client_Models=[]\n",
    "    print(\"\\n\\n---------Clustering step\", step)\n",
    "    FL_list=[]\n",
    "    client_IDs=[]\n",
    "    for cluster in clusters:\n",
    "        for Id in cluster:\n",
    "            client_IDs.append(Id)\n",
    "        cluster_initial_models=generate_initial_models(step,cluster,client_IDs,client_copy_models)\n",
    "        print(\" ---in making new FL----cluster models len:\", len(cluster_initial_models),\"cluster IDs:\", client_IDs)\n",
    "        f = FL(cluster,cluster_initial_models,FL_rounds, trainloaders, testloaders, Sensitivity_percentage)\n",
    "        FL_list.append(f)\n",
    "        for member in f.client_obj_list:\n",
    "            client_Models.append(member.net)\n",
    "        for cid in client_IDs:\n",
    "            save_torch_model(client_Models[client_IDs.index(cid)], cid)\n",
    "            save_model_param(client_Models[client_IDs.index(cid)], cid, step)\n",
    "    \n",
    "    print(\"----------------------Info before clustering-------------\")\n",
    "    print(\"model_len:\", len(client_Models))\n",
    "    print(\"Client IDS:\",client_IDs )\n",
    "    start_cluster_time = datetime.now()\n",
    "    clusters = Clustering(client_IDs, trainloaders, Sensitivity_percentage, cluster_number).Clusters\n",
    "    end_cluster_time = datetime.now()\n",
    "    exe_cluster_time = end_cluster_time - start_cluster_time\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"\\n Exe Cluster Time: {exe_cluster_time}\")\n",
    "    print(\"new clustering:\",clusters)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6072062,
     "sourceId": 9887519,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
